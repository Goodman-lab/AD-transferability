{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Knowledge transfer between different human targets in predictive toxcicology using Tanimoto similarity and machine learning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HL-yMtcn4KT"
      },
      "source": [
        "# Fingerprint generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lS6OZT_bmvI9"
      },
      "source": [
        "# -*- Fingerprinter.py -*-\n",
        "\"\"\"\n",
        "Created Jan 2019\n",
        "\n",
        "author: Elena Gelzintye / Timothy E H Allen\n",
        "Code taken from: https://github.com/teha2/chemical_toxicology/tree/master/NeuralNetworks-March2020\n",
        "\"\"\"\n",
        "#%%\n",
        "\n",
        "# Import modules\n",
        "\n",
        "import pandas as pd \n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem\n",
        "import numpy as np\n",
        "from rdkit.Chem import rdMolDescriptors\n",
        "from rdkit.Chem import MACCSkeys\n",
        "\n",
        "import os\n",
        "\n",
        "# Define paths and variables\n",
        "\n",
        "'''\n",
        "chemicals_path= binary activity file (.csv)\n",
        "fingerprints_path= location for output (.csv)\n",
        "fingerprint_length = length of genrerated fingerprint\n",
        "fingerprint_radius = radius of gernerated fingerprint\n",
        "'''\n",
        "# Define ECFP fingerprinting procedure\n",
        "\n",
        "def get_fingerprint(smiles):\n",
        "    '''smiles dataframe'''\n",
        "    \n",
        "    rdkit_molecules=[Chem.MolFromSmiles(x) for x in smiles['SMILES']]\n",
        "    rdkit_fingerprint=[]\n",
        "    count = 0\n",
        "    for mol in rdkit_molecules:\n",
        "        if count % 1000 == 0:\n",
        "            print('Now fingerprinting {} of {} for {}'.format(count,len(rdkit_molecules), receptor))\n",
        "        bit_info={}\n",
        "        fp=rdMolDescriptors.GetMorganFingerprintAsBitVect(mol, radius=fingerprint_radius, nBits=fingerprint_length, \\\n",
        "                                                                      bitInfo=bit_info).ToBitString() \n",
        "        \n",
        "        rdkit_fingerprint.append(fp)\n",
        "        count += 1\n",
        "    fingerprint_df=pd.DataFrame([np.array(list(x)).astype(int) for x in rdkit_fingerprint])\n",
        "    \n",
        "    return fingerprint_df\n",
        "\n",
        "\n",
        "# Get fingerprints\n",
        "root_dir = '/content/drive/My Drive/Applicability domains/Similarities/Similarity indicator/Machine learning/'\n",
        "filename = root_dir + 'Targets to calculate 5.csv'\n",
        "receptor_df = pd.read_csv(filename)\n",
        "receptor_ls = list(receptor_df['Target'])\n",
        "print(len(receptor_ls))\n",
        "\n",
        "#receptor_ls = ['AChE','ADORA2A','AR','hERG','SERT']\n",
        "fp_ls = [2048]\n",
        "fingerprint_radius = 2\n",
        "root_dir = '/content/drive/My Drive/Applicability domains/Similarities/Similarity indicator/Machine learning/'\n",
        "download = 'aop-wiki-xml-2021-04-01'\n",
        "for receptor in receptor_ls:\n",
        "    for length in fp_ls:\n",
        "\n",
        "        fingerprint_length = length\n",
        "        train_chemicals_path = root_dir + str(receptor) + '/' + str(receptor) + '_train.csv'\n",
        "        test_chemicals_path = '/content/drive/My Drive/AOP project/Combined data/' + str(download) + ' combined data.csv'\n",
        "        train_fingerprints_path = root_dir + str(receptor) + '/' + str(receptor) + '_train_fingerprints Morgan ' + str(length) + '.csv'\n",
        "        test_fingerprints_path = '/content/drive/My Drive/AOP project/Combined data/' + str(download) + ' combined data fingerprints Morgan ' + str(length) + '.csv'\n",
        "\n",
        "\n",
        "        #=====================================================================#\n",
        "        \n",
        "        if os.path.isfile(train_fingerprints_path) == True:\n",
        "            print('\\ngetting training fingerprints')\n",
        "            smiles=pd.read_csv(train_chemicals_path)\n",
        "\n",
        "            fingerprints=get_fingerprint(smiles)\n",
        "\n",
        "            fingerprints = pd.concat([fingerprints,smiles['Binary Activity']], axis=1)\n",
        "\n",
        "            # Outputs fingerprints\n",
        "            fingerprints.to_csv(train_fingerprints_path, index = False)\n",
        "\n",
        "        #=====================================================================#\n",
        "        \n",
        "        if os.path.isfile(test_fingerprints_path) == True:\n",
        "            print('\\ngetting test fingerprints')\n",
        "            smiles=pd.read_csv(test_chemicals_path)\n",
        "            smiles = smiles[['Smiles']]\n",
        "            smiles.columns = ['SMILES']\n",
        "\n",
        "            fingerprints=get_fingerprint(smiles)\n",
        "\n",
        "            #fingerprints = pd.concat([fingerprints,smiles['Binary Activity']], axis=1)\n",
        "\n",
        "            # Outputs fingerprints\n",
        "            fingerprints.to_csv(test_fingerprints_path, index = False)\n",
        "\n",
        "#Endgame\n",
        "\n",
        "print(\"END\")\n",
        "\n",
        "#%% "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjo6l-rzm1DC"
      },
      "source": [
        "# Modeller"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqpLjfIlQnKH"
      },
      "source": [
        "# -*- ChAIkeras.py -*-\n",
        "\"\"\"\n",
        "Created Oct 2019\n",
        "\n",
        "author: Timothy E H Allen\n",
        "Code taken from: https://github.com/teha2/chemical_toxicology/tree/master/NeuralNetworks-March2020\n",
        "\"\"\"\n",
        "#%%\n",
        "\n",
        "# Import the usual suspects\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras import regularizers\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
        "from sklearn.utils import class_weight\n",
        "import random\n",
        "\n",
        "# DEFINE INPUTS FOR MODEL TRAINING\n",
        "\n",
        "# Get test targets\n",
        "root_dir = '/content/drive/My Drive/Applicability domains/Similarities/Similarity indicator/Machine learning/'\n",
        "filename = root_dir + 'Targets to calculate 5.csv'\n",
        "receptor_df = pd.read_csv(filename)\n",
        "receptor_ls = list(receptor_df['Target'])\n",
        "#receptor_ls = ['hERG']\n",
        "\n",
        "list1 = []\n",
        "list2 = []\n",
        "list3 = []\n",
        "list4 = []\n",
        "\n",
        "# Available receptors: \n",
        "\n",
        "train_receptor = 'AChE'\n",
        "root_dir = '/content/drive/My Drive/Applicability domains/Similarities/Similarity indicator/Machine learning/'\n",
        "filename = root_dir + str(train_receptor) + '/'\n",
        "filename = filename + str(train_receptor) + '_train_fingerprints ECFP4 10000.csv'\n",
        "#input_data_training = \"/content/drive/My Drive/\" + receptor + \"_training_fingerprint.csv\"\n",
        "input_data_training = filename\n",
        "\n",
        "# filename = root_dir + str(receptor) + '/'\n",
        "# filename = filename + str(receptor) + '_test_fingerprints ECFP4 10000.csv'\n",
        "# input_data_test = filename\n",
        "\n",
        "rng_1 = random.randrange(1,1000)\n",
        "rng_2 = random.randrange(1,1000)\n",
        "validation_proportion = 0.25\n",
        "beta = 0.1\n",
        "neurons = 100\n",
        "hidden_layers = 2\n",
        "LR = 0.001\n",
        "epochs = 100\n",
        "\n",
        "\n",
        "print(\"Welcome to ChAI\")\n",
        "print(\"Dataset loading...\")\n",
        "\n",
        "# Reading The Dataset\n",
        "\n",
        "def read_dataset(input_data):\n",
        "    df = pd.read_csv(input_data)\n",
        "    X = df[df.columns[0:10000]].values\n",
        "    print(X)\n",
        "    y = df[df.columns[-1]]\n",
        "    print(y)\n",
        "    # Encode the dependent variable\n",
        "    encoder = LabelEncoder()\n",
        "    encoder.fit(y)\n",
        "    Y = encoder.transform(y)\n",
        "    #print(\"X.shape =\", X.shape)\n",
        "    #print(\"Y.shape =\", Y.shape)\n",
        "    #print(\"y.shape =\", y.shape)\n",
        "    return (X, Y)\n",
        "\n",
        "X, Y = read_dataset(input_data_training)\n",
        "\n",
        "# # Get one molecule from test only\n",
        "# single_mol = 14\n",
        "# single_test_x = test_x[single_mol:single_mol+1,:]\n",
        "# # print(single_test_x.shape)\n",
        "# # print(single_test_x)\n",
        "# single_test_y = test_y[single_mol:single_mol+1]\n",
        "# # print(single_test_y.shape)\n",
        "# # print(single_test_y)\n",
        "\n",
        "\n",
        "\n",
        "# Shuffle the dataset\n",
        "\n",
        "X, Y = shuffle(X, Y, random_state=rng_1)\n",
        "\n",
        "# Convert the dataset into train and validation sets\n",
        "\n",
        "train_x, valid_x, train_y, valid_y = train_test_split(X, Y, test_size =validation_proportion, random_state=rng_2)\n",
        "\n",
        "# Inspect the shape of the training and validation data\n",
        "\n",
        "print(\"Dimensionality of data:\")\n",
        "print(\"Train x shape =\", train_x.shape)\n",
        "print(\"Train y shape =\", train_y.shape)\n",
        "print(\"Validation x shape =\", valid_x.shape)\n",
        "print(\"Validation y shape =\", valid_y.shape)\n",
        "\n",
        "class_weights = class_weight.compute_class_weight('balanced',\n",
        "                                                np.unique(train_y),\n",
        "                                                train_y)\n",
        "\n",
        "#Define the model in keras\n",
        "\n",
        "print(\"Constructing model architecture\")\n",
        "\n",
        "if hidden_layers == 1:\n",
        "    inputs = keras.Input(shape=(10000,), name='digits')\n",
        "    x = layers.Dense(neurons, activation='relu', kernel_initializer='random_uniform', bias_initializer='zeros', kernel_regularizer=regularizers.l2(beta), name='dense_1')(inputs)\n",
        "    outputs = layers.Dense(2, activation='softmax', name='predictions')(x)\n",
        "elif hidden_layers == 2:\n",
        "    inputs = keras.Input(shape=(10000,), name='digits')\n",
        "    x = layers.Dense(neurons, activation='relu', kernel_initializer='random_uniform', bias_initializer='zeros', kernel_regularizer=regularizers.l2(beta), name='dense_1')(inputs)\n",
        "    x = layers.Dense(neurons, activation='relu', kernel_initializer='random_uniform', bias_initializer='zeros', kernel_regularizer=regularizers.l2(beta), name='dense_2')(x)\n",
        "    outputs = layers.Dense(2, activation='softmax', name='predictions')(x)\n",
        "elif hidden_layers == 3:\n",
        "    inputs = keras.Input(shape=(10000,), name='digits')\n",
        "    x = layers.Dense(neurons, activation='relu', kernel_initializer='random_uniform', bias_initializer='zeros', kernel_regularizer=regularizers.l2(beta), name='dense_1')(inputs)\n",
        "    x = layers.Dense(neurons, activation='relu', kernel_initializer='random_uniform', bias_initializer='zeros', kernel_regularizer=regularizers.l2(beta), name='dense_2')(x)\n",
        "    x = layers.Dense(neurons, activation='relu', kernel_initializer='random_uniform', bias_initializer='zeros', kernel_regularizer=regularizers.l2(beta), name='dense_3')(x)\n",
        "    outputs = layers.Dense(2, activation='softmax', name='predictions')(x)\n",
        "else:\n",
        "    print(\"Number of hidden layers outside this model scope, please choose 1, 2 or 3\")\n",
        "\n",
        "model = keras.Model(inputs = inputs, outputs = outputs)\n",
        "\n",
        "model.compile(optimizer=keras.optimizers.Adam(lr=LR),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['sparse_categorical_accuracy'])\n",
        "\n",
        "print('Commencing model training...')\n",
        "history = model.fit(train_x, train_y,\n",
        "                    batch_size=128,\n",
        "                    epochs=epochs,\n",
        "                    class_weight=class_weights,\n",
        "                    # We pass some validation for\n",
        "                    # monitoring validation loss and metrics\n",
        "                    # at the end of each epoch\n",
        "                    validation_data=(valid_x, valid_y))\n",
        "\n",
        "# The returned \"history\" object holds a record\n",
        "# of the loss values and metric values during training\n",
        "\n",
        "# Evaluate the model on the training and validation data\n",
        "print('\\n# Evaluate on training data')\n",
        "train_results = model.evaluate(train_x, train_y, batch_size=128)\n",
        "print('train loss, train acc:', train_results)\n",
        "\n",
        "print('\\n# Evaluate on validation data')\n",
        "validation_results = model.evaluate(valid_x, valid_y, batch_size=128)\n",
        "print('validation loss, validation acc:', validation_results)\n",
        "\n",
        "# Save the model\n",
        "for receptor in receptor_ls:\n",
        "    count = 0\n",
        "    model_path = root_dir + str(train_receptor) + '/Models vs other targets/Models/' + str(receptor) + '_model.h5'\n",
        "    model.save(model_path)\n",
        "    print('Model saved to ' + model_path)\n",
        "    root_dir = '/content/drive/My Drive/Applicability domains/Similarities/Similarity indicator/Machine learning/'\n",
        "    filename = root_dir + str(receptor) + '/'\n",
        "    filename = filename + str(receptor) + '_test_fingerprints ECFP4 10000.csv'\n",
        "    input_data_test = filename\n",
        "    test_x, test_y = read_dataset(input_data_test)\n",
        "\n",
        "    \n",
        "    pred_valid_y = model.predict(valid_x, verbose=1)\n",
        "    pred_train_y = model.predict(train_x, verbose=1)\n",
        "    pred_test_y = model.predict(test_x)\n",
        "\n",
        "    # Define experimental and predicted values using argmax\n",
        "    \n",
        "    pred_train_y_binary = np.argmax(pred_train_y, axis=1)\n",
        "    pred_valid_y_binary = np.argmax(pred_valid_y, axis=1)\n",
        "    pred_test_y_binary = np.argmax(pred_test_y, axis=1)\n",
        "\n",
        "    # Calculate and display confusion matricies\n",
        "    def get_accuracy(cm):\n",
        "        TP = cm[0][0]\n",
        "        FP = cm[0][1]\n",
        "        FN = cm[1][0]\n",
        "        TN = cm[1][1]\n",
        "\n",
        "        accuracy = (TP + TN) / (TP + FP + FN + TN)\n",
        "\n",
        "        return accuracy\n",
        "\n",
        "    \n",
        "    cm = confusion_matrix(train_y, pred_train_y_binary)\n",
        "    np.set_printoptions(precision=2)\n",
        "    #print(\"Confusion matrix (Training), without normalisation\")\n",
        "    #print(cm)\n",
        "    train_accuracy = get_accuracy(cm)\n",
        "\n",
        "    cm = confusion_matrix(valid_y, pred_valid_y_binary)\n",
        "    np.set_printoptions(precision=2)\n",
        "    #print(\"Confusion matrix (Validation), without normalisation\")\n",
        "    #print(cm)\n",
        "    valid_accuracy = get_accuracy(cm)\n",
        "\n",
        "    cm = confusion_matrix(test_y, pred_test_y_binary)\n",
        "    np.set_printoptions(precision=2)\n",
        "    #print(\"Confusion matrix (Test), without normalisation\")\n",
        "    #print(cm)\n",
        "    test_accuracy = get_accuracy(cm)\n",
        "\n",
        "    # Append all values to lists\n",
        "    list1.append(receptor)\n",
        "    list2.append(train_accuracy)\n",
        "    list3.append(valid_accuracy)\n",
        "    list4.append(test_accuracy)\n",
        "\n",
        "    # Get dataframe of accuracies\n",
        "    accuracy_df = pd.DataFrame(list1,columns = ['Receptor'])\n",
        "    accuracy_df['Train_acc'] = list2\n",
        "    accuracy_df['Valid_acc'] = list3\n",
        "    accuracy_df['Test_acc'] = list4\n",
        "    print(accuracy_df)\n",
        "\n",
        "    root_dir = '/content/drive/My Drive/Applicability domains/Similarities/Similarity indicator/Machine learning/'\n",
        "    filename = root_dir + str(train_receptor) + '/Accuracies/'\n",
        "    filename = filename + '_accuracies2 ECFP4 10000.csv'\n",
        "    accuracy_df.to_csv(filename)\n",
        "\n",
        "\n",
        "\n",
        "print(\"END\")\n",
        "\n",
        "\n",
        "# approx 5-10 min per model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xE10f3C_lULM"
      },
      "source": [
        "# For calculating similarities of specified targets vs all 79 targets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAR4zBqhfJ7e"
      },
      "source": [
        "\"\"\"\n",
        "Created Jun 2021\n",
        "\n",
        "Author: Marcus Wei How Wang\n",
        "Code available at: https://github.com/Goodman-lab/AD-transferability\n",
        "Please acknowledge the authors if using the code, whether partially or in full\n",
        "\"\"\"\n",
        "\n",
        "# For use with model vs other targets from Tim's code\n",
        "# LATEST CODE UPDATED 20 MAR 2021\n",
        "# Code for calculating applicability domain metric based on Tanimoto similarity\n",
        "\n",
        "%%time\n",
        "# Note google colab disconnects and clears data after 12 hrs of inactivity\n",
        "# But code still runs in the background even if runtime is disconnected before the 12hr mark\n",
        "# !pip install -I tensorflow\n",
        "# !pip install -I keras\n",
        "# Relevant imports\n",
        "import numpy as np\n",
        "import pandas as pd # uses pandas python module to view and analyse data\n",
        "\n",
        "import time\n",
        "from time import strftime, gmtime\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import itertools \n",
        "\n",
        "import os\n",
        "from os import mkdir\n",
        "\n",
        "from rdkit import Chem\n",
        "from rdkit import DataStructs\n",
        "\n",
        "from rdkit.Chem import AllChem\n",
        "from rdkit.Chem import Draw\n",
        "from rdkit.Chem import rdMolDescriptors\n",
        "from rdkit.Chem import PandasTools\n",
        "from rdkit.Chem.Draw.MolDrawing import MolDrawing\n",
        "from rdkit.Chem.Draw import IPythonConsole\n",
        "from rdkit.Chem import MACCSkeys\n",
        "from rdkit.Chem.rdMolDescriptors import GetMACCSKeysFingerprint\n",
        "\n",
        "#=======================================================================================#\n",
        "# READ REQUIRED FILES\n",
        "# Files contain the SMILES string\n",
        "\n",
        "# Read csv file containing targets to calculate\n",
        "filename = '/content/drive/My Drive/Applicability domains/Similarities/Similarity indicator/Machine learning/'\n",
        "#filename = filename + 'Targets to calculate 5.csv'\n",
        "filename = filename + 'Targets for optimisation.csv'\n",
        "\n",
        "target_df = pd.read_csv(filename)\n",
        "\n",
        "target_df = target_df[['Target']]\n",
        "print(target_df)\n",
        "\n",
        "error_ls = []\n",
        "error_ls.clear()\n",
        "\n",
        "train_target_ls = ['AChE']\n",
        "\n",
        "runs = 3\n",
        "\n",
        "#=======================================================================================#\n",
        "# Specify filenames for main code\n",
        "# Training fingerprints\n",
        "def train_fp_function():\n",
        "    train_fp_filename = '/content/drive/My Drive/Applicability domains/Similarities/Similarity indicator/Machine learning/'\n",
        "    train_fp_filename = train_fp_filename + str(train_target) + '/'\n",
        "    train_fp_filename = train_fp_filename + str(train_target) + '_train_fingerprints ECFP4 10000.csv'\n",
        "    return train_fp_filename\n",
        "\n",
        "# Folder to contain all data for test targets\n",
        "# Each target is put in an individual folder\n",
        "def test_target_folder_function():\n",
        "    root_dir = '/content/drive/My Drive/Applicability domains/Similarities/Similarity indicator/Machine learning/'\n",
        "    root_dir = root_dir + str(train_target) + '/Models vs other targets/'\n",
        "    foldername = target\n",
        "    test_target_folder = root_dir + foldername\n",
        "    return test_target_folder\n",
        "\n",
        "# Test fingerprints\n",
        "def test_fp_function():\n",
        "    test_fp_filename = '/content/drive/My Drive/Applicability domains/Similarities/Similarity indicator/Machine learning/'\n",
        "    test_fp_filename = test_fp_filename + str(target) + '/'\n",
        "    test_fp_filename = test_fp_filename + str(target) + '_test_fingerprints ECFP4 10000.csv'\n",
        "    return test_fp_filename\n",
        "\n",
        "# Save file containing train,test,average similarity indicator values per target\n",
        "def indicator_function():\n",
        "    root_dir = '/content/drive/My Drive/Applicability domains/Similarities/Similarity indicator/Machine learning/'\n",
        "    root_dir = root_dir + str(train_target) + '/Models vs other targets/'\n",
        "    indicator_filename = root_dir  + str(target) + '/'\n",
        "    indicator_filename = indicator_filename + str(target) + '_' + 'NEW_' + str(sample_size) + 'data_' + str(runs) + '_'\n",
        "    indicator_filename = indicator_filename + str(nBits) + 'bits_' + str(T_sim_threshold) + 'T_threshold_' + str(mol_threshold) + 'mol_sim_indicator.csv'\n",
        "    return indicator_filename\n",
        "\n",
        "# Create folder to contain combined indicator values\n",
        "def combined_indicator_folder_function():\n",
        "    root_dir = '/content/drive/My Drive/Applicability domains/Similarities/Similarity indicator/Machine learning/'\n",
        "    root_dir = root_dir + str(train_target) + '/Models vs other targets/'\n",
        "    foldername = str(target) + '/Combined'\n",
        "    combined_indicator_foldername = root_dir + foldername\n",
        "    return combined_indicator_foldername\n",
        "\n",
        "# Combined indicator values file per target for all thresholds\n",
        "def combined_indicator_function():\n",
        "    root_dir = '/content/drive/My Drive/Applicability domains/Similarities/Similarity indicator/Machine learning/'\n",
        "    root_dir = root_dir + str(train_target) + '/Models vs other targets/'\n",
        "    combined_indicator_filename = root_dir + str(target) + '/Combined/'\n",
        "    combined_indicator_filename = combined_indicator_filename + str(target) + '_Combined_' + 'NEW_' + str(sample_size) + 'data_' + str(runs) + '_'\n",
        "    combined_indicator_filename = combined_indicator_filename + str(nBits) + 'bits_' + str(T_sim_threshold) + 'T_threshold_' + str(mol_threshold) + 'mol_sim_indicator.csv'\n",
        "    return combined_indicator_filename\n",
        "\n",
        "# For saving all results in code\n",
        "def all_results_function():\n",
        "    all_results_filename = '/content/drive/My Drive/Applicability domains/Similarities/Similarity indicator/Machine learning/'\n",
        "    all_results_filename = all_results_filename + str(train_target) + '/Models vs other targets/'    \n",
        "    all_results_filename = all_results_filename + '- All results/' + 'all_df_' + 'NEW_' + str(sample_size) + 'data_' + str(runs) + '_'\n",
        "    all_results_filename = all_results_filename + str(nBits) + 'bits_' + str(T_sim_threshold) + 'T_threshold_' + str(mol_threshold) + 'mol_sim_indicator.csv'\n",
        "    return all_results_filename\n",
        "\n",
        "#=======================================================================================#\n",
        "\n",
        "# Main code\n",
        "for train_target in train_target_ls:\n",
        "    sim_ls = []\n",
        "    sim_ls.clear()\n",
        "\n",
        "    mol_threshold_ls = []\n",
        "    mol_threshold_ls.clear()\n",
        "\n",
        "    temp_ls = []\n",
        "    temp_ls.clear()\n",
        "    target_count = 0\n",
        "\n",
        "    # Read file 1 (training dataset)\n",
        "    print ('\\nReading file 1...')\n",
        "    print ('Current time:')\n",
        "    print(strftime(\"%H:%M:%S\", gmtime())) \n",
        "    start = time.time()\n",
        "\n",
        "    train_fp_df = pd.read_csv(train_fp_function())\n",
        "\n",
        "    train_fp_df = train_fp_df.sample(frac=1)\n",
        "    train_fp_df = train_fp_df.reset_index(drop=True)\n",
        "\n",
        "    #print (train_fp_df)\n",
        "    print (train_fp_df.shape)\n",
        "\n",
        "    train_fp = train_fp_df.iloc[:,0:10000]\n",
        "    train_fp['combined'] = train_fp[train_fp.columns.tolist()].apply(lambda row: ''.join(row.values.astype(str)), axis=1)\n",
        "    train_fp = train_fp[['combined']]\n",
        "    train_fp_active = train_fp_df.iloc[:,-1:]\n",
        "\n",
        "    end = time.time()\n",
        "    elapsed = end - start\n",
        "    minutes = elapsed // 60\n",
        "    seconds = elapsed - (minutes*60)\n",
        "    print('File 1 took {} minutes and {} seconds to read'.format(minutes,seconds))\n",
        "\n",
        "    for protein in range(0,len(target_df)):\n",
        "        \n",
        "        target = str(target_df.loc[protein]['Target'])\n",
        "        print('For train_target: {}'.format(train_target))\n",
        "        print('Processing target: {}'.format(protein))\n",
        "        print('Processing target: {}'.format(target))\n",
        "        \n",
        "        # Create folders\n",
        "        if os.path.isdir(test_target_folder_function()) == False:\n",
        "            os.mkdir(test_target_folder_function())\n",
        "\n",
        "        #if target != train_target:\n",
        "        if True:\n",
        "\n",
        "            # Read file 2 (test dataset)\n",
        "            print ('\\nReading file 2...')\n",
        "            print ('Current time:')\n",
        "            print(strftime(\"%H:%M:%S\", gmtime())) \n",
        "            start = time.time()\n",
        "                \n",
        "            test_fp_df = pd.read_csv(test_fp_function())\n",
        "\n",
        "            test_fp_df = test_fp_df.sample(frac=1)\n",
        "            test_fp_df = test_fp_df.reset_index(drop=True)\n",
        "\n",
        "            #print (test_fp_df)\n",
        "            print (test_fp_df.shape)\n",
        "\n",
        "            test_fp = test_fp_df.iloc[:,0:10000]\n",
        "            test_fp['combined'] = test_fp[test_fp.columns.tolist()].apply(lambda row: ''.join(row.values.astype(str)), axis=1)\n",
        "            test_fp = test_fp[['combined']]\n",
        "            test_fp_active = test_fp_df.iloc[:,-1:]\n",
        "\n",
        "            end = time.time()\n",
        "            elapsed = end - start\n",
        "            minutes = elapsed // 60\n",
        "            seconds = elapsed - (minutes*60)\n",
        "            print('File 2 took {} minutes and {} seconds to read'.format(minutes,seconds))\n",
        "\n",
        "            #=========================================================================================#\n",
        "\n",
        "            start = time.time()\n",
        "            print('\\nPROCESSING AND SAMPLING DATA FROM BOTH FILES...')\n",
        "\n",
        "            # IF SAMPLING\n",
        "            # Get count of data points for both datasets\n",
        "            # training_sample_fp = train_fp[0:500]\n",
        "            # training_sample_active = train_fp_active[0:500]\n",
        "            # test_sample_fp = test_fp[0:500]\n",
        "            # test_sample_active = test_fp_active[0:500]\n",
        "\n",
        "            # Get count of data points for both datasets\n",
        "            sample_size = 500\n",
        "            training_sample_fp = train_fp[0:sample_size]\n",
        "            training_sample_active = train_fp_active[0:sample_size]\n",
        "            test_sample_fp = test_fp[0:sample_size]\n",
        "            test_sample_active = test_fp_active[0:sample_size]\n",
        "\n",
        "            # IF NOT SAMPLING\n",
        "            # Get count of data points for both datasets\n",
        "            # training_sample_fp = train_fp\n",
        "            # training_sample_active = train_fp_active\n",
        "            # test_sample_fp = test_fp\n",
        "            # test_sample_active = test_fp_active\n",
        "\n",
        "            train_count_ls = []\n",
        "            test_count_ls = []\n",
        "\n",
        "            print('\\nFILES PROCESSED AND SAMPLED')\n",
        "            #=========================================================================================#\n",
        "            start = time.time()\n",
        "            print('\\nCREATING BITVECT DFs FOR BOTH TRAINING AND TEST SAMPLES...')\n",
        "            train_bit_ls = []\n",
        "            test_bit_ls = []\n",
        "\n",
        "            train_bit_ls.clear()\n",
        "            test_bit_ls.clear()\n",
        "\n",
        "            for x in range(0,len(training_sample_fp)):\n",
        "                training_bitvect = DataStructs.CreateFromBitString(training_sample_fp.iloc[x]['combined'])\n",
        "                train_bit_ls.append(training_bitvect)\n",
        "            for y in range(0,len(test_sample_fp)):\n",
        "                test_bitvect = DataStructs.CreateFromBitString(test_sample_fp.iloc[y]['combined'])\n",
        "                test_bit_ls.append(test_bitvect)\n",
        "\n",
        "            train_bv_df = pd.DataFrame(train_bit_ls,columns = ['BV'])\n",
        "            test_bv_df = pd.DataFrame(test_bit_ls,columns = ['BV'])\n",
        "\n",
        "            #print(train_bv_df)\n",
        "            #print(test_bv_df)\n",
        "\n",
        "            print('\\nBITVECT DFs CREATED')\n",
        "            #=========================================================================================#\n",
        "\n",
        "            print('\\nCALCULATING TRAINING TANIMOTO SIMILARITIES...')\n",
        "\n",
        "            # Set similarity thresholds and no. of similar molecules threshold\n",
        "            T_ls = [0.2]\n",
        "            mol_ls = [1]\n",
        "            nBits = 10000\n",
        "\n",
        "            for element in T_ls:\n",
        "                \n",
        "                T_sim_threshold = float(element)\n",
        "\n",
        "                for ele in mol_ls:\n",
        "                    print('\\nNOW CALCULATING FOR THRESHOLD: {}'.format(element))\n",
        "                    print('\\nNOW CALCULATING FOR nMOL: {}'.format(ele))\n",
        "                    print('\\nNOW CALCULATING FOR TARGET: {}'.format(target))\n",
        "\n",
        "                    sim_count = 0\n",
        "                    indicator_ls = []\n",
        "                    indicator_ls.clear()\n",
        "\n",
        "                    extract_index = []\n",
        "                    extract_index.clear()\n",
        "\n",
        "                    mol_threshold = int(ele)\n",
        "                    # Calculate Tanimoto similarity between both samples\n",
        "                    for x in range(0,len(train_bv_df)):\n",
        "                        train_bv = train_bv_df.iloc[x]['BV']\n",
        "                        training_active_state = training_sample_active.iloc[x]['Binary Activity']\n",
        "                        if x % 100 == 0:\n",
        "                            print ('Current time:')\n",
        "                            print(strftime(\"%H:%M:%S\", gmtime()))\n",
        "                            print('NOW CALCULATING SIMILARITIES FOR TRAINING INDEX {}'.format(x))\n",
        "\n",
        "\n",
        "                        for y in range(0,len(test_bv_df)):\n",
        "                            temp_count = 0\n",
        "                            test_bv = test_bv_df.iloc[y]['BV']\n",
        "                            sim = DataStructs.TanimotoSimilarity(train_bv,test_bv)\n",
        "                            test_active_state = test_sample_active.iloc[y]['Binary Activity']\n",
        "\n",
        "                            # Process sim per molecule and determine if molecule is similar to second dataset                            \n",
        "                            if sim >= T_sim_threshold:\n",
        "                                if test_active_state == training_active_state:\n",
        "                                    temp_count += 1\n",
        "                        \n",
        "                                    if temp_count >= mol_threshold:\n",
        "                                        sim_count += 1\n",
        "                                        break\n",
        "                          \n",
        "                    print('\\nTRAINING TANIMOTO SIMILARITIES CALCULATED')\n",
        "\n",
        "                    #=========================================================================================#\n",
        "                    print('\\nCALCULATING TRAINING SIMILARITY INDICATOR...')\n",
        "\n",
        "                    train_indicator = sim_count / len(train_bv_df) * 100\n",
        "                    indicator_ls.append(train_indicator)\n",
        "\n",
        "                    print('\\nTRAINING SIMILARITY INDICATOR CALCULATED')\n",
        "                    #=========================================================================================#\n",
        "\n",
        "                    print('\\nCALCULATING TEST TANIMOTO SIMILARITIES...')\n",
        "                    \n",
        "                    # Calculate Tanimoto similarity between both samples\n",
        "                    sim_count = 0\n",
        "                    for x in range(0,len(test_bv_df)):\n",
        "                        test_bv = test_bv_df.iloc[x]['BV']\n",
        "                        test_active_state = test_sample_active.iloc[x]['Binary Activity']\n",
        "                        if x % 100 == 0:\n",
        "                            print ('Current time:')\n",
        "                            print(strftime(\"%H:%M:%S\", gmtime()))\n",
        "                            print('NOW CALCULATING SIMILARITIES FOR TRAINING INDEX {}'.format(x))\n",
        "\n",
        "\n",
        "                        for y in range(0,len(train_bv_df)):\n",
        "                            temp_count = 0\n",
        "                            train_bv = train_bv_df.iloc[y]['BV']\n",
        "                            sim = DataStructs.TanimotoSimilarity(train_bv,test_bv)\n",
        "                            training_active_state = training_sample_active.iloc[y]['Binary Activity']\n",
        "\n",
        "                            # Process sim per molecule and determine if molecule is similar to second dataset                            \n",
        "                            if sim >= T_sim_threshold:\n",
        "                                if test_active_state == training_active_state:\n",
        "                                    temp_count += 1\n",
        "                        \n",
        "                                    if temp_count >= mol_threshold:\n",
        "                                        sim_count += 1\n",
        "                                        break\n",
        "                          \n",
        "                    print('\\nTEST TANIMOTO SIMILARITIES CALCULATED')\n",
        "                    #=========================================================================================#\n",
        "                    print('\\nCALCULATING TEST SIMILARITY INDICATOR...')\n",
        "\n",
        "                    test_indicator = sim_count / len(test_bv_df) * 100\n",
        "                    indicator_ls.append(test_indicator)\n",
        "\n",
        "                    print('\\nTEST SIMILARITY INDICATOR CALCULATED')\n",
        "\n",
        "                    #=========================================================================================#\n",
        "                    print('\\nCALCULATING AVERAGE SIMILARITY INDICATOR...')\n",
        "\n",
        "                    average_indicator = (train_indicator + test_indicator) / 2\n",
        "                    indicator_ls.append(average_indicator)\n",
        "                    \n",
        "\n",
        "                    indicator_df = pd.DataFrame(indicator_ls).T\n",
        "                    indicator_df.columns = ['Train','Test','Average']\n",
        "\n",
        "                    print(indicator_df)\n",
        "\n",
        "                    print('\\nAVERAGE SIMILARITY INDICATOR CALCULATED')\n",
        "\n",
        "                    #=========================================================================================#\n",
        "\n",
        "                    print ('\\nSAVING FILES...')\n",
        "                    start = time.time()\n",
        "                    print ('Current time:')\n",
        "                    print(strftime(\"%H:%M:%S\", gmtime()))\n",
        "\n",
        "                    indicator_df.to_csv(indicator_function())\n",
        "\n",
        "                    end = time.time()\n",
        "                    elapsed = end - start\n",
        "                    minutes = elapsed // 60\n",
        "                    seconds = elapsed - (minutes*60)\n",
        "                    print('Files took {} minutes and {} seconds to save'.format(minutes,seconds))\n",
        "                    print ('\\n#=========================================================================#')\n",
        "                    train_count_ls.append(len(training_sample_fp))\n",
        "                    test_count_ls.append(len(test_sample_fp))\n",
        "\n",
        "            # Collate all indicator files/results\n",
        "            print('\\nCOLLATING ALL FILES...')\n",
        "            f_count = 0\n",
        "\n",
        "            sim_ls = []\n",
        "            sim_ls.clear()\n",
        "\n",
        "            mol_threshold_ls = []\n",
        "            mol_threshold_ls.clear()\n",
        "\n",
        "            temp_ls = []\n",
        "            temp_ls.clear()\n",
        "\n",
        "            for element in T_ls:\n",
        "                T_sim_threshold = float(element)\n",
        "\n",
        "                for ele in mol_ls:\n",
        "                    mol_threshold = int(ele)\n",
        "                  \n",
        "                    # Read file \n",
        "                    print ('\\nReading file ...')\n",
        "                    print ('Current time:')\n",
        "                    print(strftime(\"%H:%M:%S\", gmtime())) \n",
        "                    start = time.time()\n",
        "\n",
        "                    file1 = pd.read_csv(indicator_function())\n",
        "                    file1mod = file1.drop(['Unnamed: 0'], axis=1)\n",
        "\n",
        "                    end = time.time()\n",
        "                    elapsed = end - start\n",
        "                    minutes = elapsed // 60\n",
        "                    seconds = elapsed - (minutes*60)\n",
        "                    print('File 1 took {} minutes and {} seconds to read'.format(minutes,seconds))\n",
        "\n",
        "                    if f_count == 0:\n",
        "                        combined_df = file1mod\n",
        "                        f_count += 1\n",
        "\n",
        "                    else:\n",
        "                        combined_df = pd.concat([combined_df,file1mod],axis = 0)\n",
        "                        \n",
        "                    sim_ls.append(T_sim_threshold)\n",
        "                    mol_threshold_ls.append(mol_threshold)\n",
        "                    temp_ls.append(target)\n",
        "\n",
        "            print('\\nALL FILES COLLATED')\n",
        "            #===========================================================================#\n",
        "            # Process df and save\n",
        "\n",
        "            print('\\nSAVING DF FOR TARGET {}'.format(protein))\n",
        "            combined_df['Sim'] = sim_ls\n",
        "            combined_df['No. of mol'] = mol_threshold_ls\n",
        "            combined_df['Target'] = temp_ls\n",
        "            combined_df['train_target count'] = train_count_ls\n",
        "            combined_df['test_target count'] = test_count_ls\n",
        "\n",
        "            print(combined_df)\n",
        "\n",
        "            print ('\\nSAVING FILES...')\n",
        "            start = time.time()\n",
        "            print ('Current time:')\n",
        "            print(strftime(\"%H:%M:%S\", gmtime()))\n",
        "\n",
        "            # Create folder in google drive\n",
        "            if os.path.isdir(combined_indicator_folder_function()) == False:\n",
        "                os.mkdir(combined_indicator_folder_function())\n",
        "\n",
        "            combined_df.to_csv(combined_indicator_function())\n",
        "            \n",
        "            if target_count == 0:\n",
        "\n",
        "                #print('FIRST LINE')\n",
        "                all_df = combined_df\n",
        "                target_count += 1\n",
        "            else:\n",
        "                #print('NEXT LINES')\n",
        "                all_df = pd.concat([all_df,combined_df],axis=0)\n",
        "\n",
        "            end = time.time()\n",
        "            elapsed = end - start\n",
        "            minutes = elapsed // 60\n",
        "            seconds = elapsed - (minutes*60)\n",
        "            print('Files took {} minutes and {} seconds to save'.format(minutes,seconds))\n",
        "            print ('\\n#=========================================================================#')\n",
        "\n",
        "            # Save all_df (combined df with all calculated targets for easy copying)\n",
        "            print('\\nSAVING ALL RESULTS FILE...')\n",
        "\n",
        "            all_df.to_csv(all_results_function())\n",
        "            print('\\nALL RESULTS FILE SAVED')\n",
        "\n",
        "print('\\nFINISHED')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntFb4LS4yVP7"
      },
      "source": [
        "# For calculating similarities of specified targets vs all 79 targets (with Multiprocessing)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVn_x-A3yQ7T"
      },
      "source": [
        "\"\"\"\n",
        "Created Jun 2021\n",
        "\n",
        "Author: Marcus Wei How Wang\n",
        "Code available at: https://github.com/Goodman-lab/AD-transferability\n",
        "Please acknowledge the authors if using the code, whether partially or in full\n",
        "\"\"\"\n",
        "# For use with model vs other targets from Tim's code\n",
        "# LATEST CODE UPDATED 29 Apr 2021\n",
        "# Code for calculating applicability domain metric based on Tanimoto similarity\n",
        "\n",
        "%%time\n",
        "# Note google colab disconnects and clears data after 12 hrs of inactivity\n",
        "# But code still runs in the background even if runtime is disconnected before the 12hr mark\n",
        "\n",
        "# Relevant imports\n",
        "import numpy as np\n",
        "import pandas as pd # uses pandas python module to view and analyse data\n",
        "\n",
        "import time\n",
        "from time import strftime, gmtime\n",
        "\n",
        "import itertools\n",
        "\n",
        "import multiprocessing\n",
        "from multiprocessing import Process, Queue\n",
        "from multiprocessing import Pool\n",
        "\n",
        "import os\n",
        "from os import mkdir\n",
        "\n",
        "from rdkit import Chem\n",
        "from rdkit import DataStructs\n",
        "\n",
        "from rdkit.Chem import AllChem\n",
        "\n",
        "\n",
        "#=======================================================================================#\n",
        "# READ REQUIRED FILES\n",
        "print('\\nSETTING UP TARGETS...')\n",
        "# Files contain the fingerprints and labels\n",
        "\n",
        "# Set test targets\n",
        "# Read csv file containing targets to calculate\n",
        "filename = '/content/drive/My Drive/Applicability domains/Similarities/Similarity indicator/Machine learning/'\n",
        "filename = filename + 'Targets to calculate 5.csv'\n",
        "#filename = filename + 'Targets for optimisation.csv'\n",
        "\n",
        "target_df = pd.read_csv(filename)\n",
        "target_df = target_df[['Target']]\n",
        "print(target_df)\n",
        "\n",
        "# Set training targets\n",
        "#train_target_ls = ['AChE','ADORA2A','AR','hERG','SERT']\n",
        "#train_target_ls = ['ADORA2A','AR','hERG','SERT']\n",
        "#train_target_ls = ['AChE']\n",
        "#train_target_ls = ['ADORA2A']\n",
        "\n",
        "train_target_ls = ['SERT']\n",
        "checkpoint_target = 'SERT'\n",
        "state = 'Yes'\n",
        "\n",
        "#train_target_ls = ['hERG']\n",
        "#train_target_ls = ['hERG','SERT']\n",
        "#train_target_ls = ['SERT']\n",
        "\n",
        "#===================================================================================================#\n",
        "# Define some functions\n",
        "\n",
        "# Function splits a df into actives and inactives and returns two dfs\n",
        "def active_inactive_split(df):\n",
        "    #print('\\nactive_inactive_split CALLED')\n",
        "\n",
        "    active_index = []\n",
        "    active_index.clear()\n",
        "\n",
        "    # Get index of actives\n",
        "    temp = df.iloc[:,-1:]\n",
        "    temp_df = df\n",
        "    for x in range(0,len(temp)):\n",
        "        if temp.iloc[x]['Binary Activity'] == 1:\n",
        "            active_index.append(x)\n",
        "    \n",
        "    # Seperate actives and inactives intwo two dfs\n",
        "    #print('\\nCREATING ACTIVE DF...')\n",
        "    active_df = temp_df.iloc[active_index]    \n",
        "    active_df = active_df.reset_index(drop=True)\n",
        "\n",
        "    #print('\\nCREATING INACTIVE DF...')\n",
        "    inactive_df = df.drop(df.index[[active_index]])\n",
        "    inactive_df = inactive_df.reset_index(drop=True)\n",
        "    \n",
        "    return active_df, inactive_df\n",
        "\n",
        "def feature_label_split(df):\n",
        "    #print('\\nfeature_label_split CALLED')\n",
        "\n",
        "    # Combine all columns with fingerprint values into format suitable for bitvector processing\n",
        "    fp = df.iloc[:,0:10000]\n",
        "    fp['combined'] = fp[fp.columns.tolist()].apply(lambda row: ''.join(row.values.astype(str)), axis=1)\n",
        "    fp = fp[['combined']]\n",
        "\n",
        "    # Get active states as df\n",
        "    fp_label = df.iloc[:,-1:]\n",
        "\n",
        "    return fp, fp_label\n",
        "\n",
        "def CreateBitVect(df):\n",
        "    #print('\\nCreateBitVect')\n",
        "\n",
        "    df['BV'] = df.apply(lambda row : DataStructs.CreateFromBitString(row['combined']), axis = 1)\n",
        "    df = df[['BV']]\n",
        "    return df\n",
        "\n",
        "def Calc_Similarity(bv1,bv2,label1,label2,que,process):\n",
        "    #print('\\nCalc_Similarity CALLED')\n",
        "    # Calculate Tanimoto similarity between both samples\n",
        "    sim_count = 0\n",
        "\n",
        "    for x in range(0,len(bv1)):\n",
        "        bv1_bv = bv1.iloc[x]['BV']\n",
        "        label1_label = label1.iloc[x]['Binary Activity']\n",
        "\n",
        "        if x % 500 == 0:\n",
        "            print ('Current time:')\n",
        "            print(strftime(\"%H:%M:%S\", gmtime()))\n",
        "            print('NOW CALCULATING SIMILARITIES FOR PROCESS {} INDEX {}'.format(process,x))\n",
        "\n",
        "        for y in range(0,len(bv2)):\n",
        "\n",
        "            bv2_bv = bv2.iloc[y]['BV']\n",
        "            sim = DataStructs.TanimotoSimilarity(bv1_bv,bv2_bv)\n",
        "            label2_label = label2.iloc[y]['Binary Activity']\n",
        "\n",
        "            # Process sim per molecule and determine if molecule is similar to second dataset                            \n",
        "            if sim >= T_sim_threshold:\n",
        "                if label1_label == label2_label:        \n",
        "                    sim_count += 1\n",
        "                    break\n",
        "    \n",
        "    #print('\\nTANIMOTO SIMILARITIES CALCULATED')\n",
        "    que.put(sim_count)\n",
        "    return sim_count\n",
        "\n",
        "#===================================================================================================#\n",
        "# IF starting from incomplete file/data ie. checkpoint:\n",
        "\n",
        "def Checkpoint(checkpoint_target, state):\n",
        "    nBits = 10000\n",
        "    T_sim_threshold = 0.3\n",
        "\n",
        "    if state == 'Yes':\n",
        "        filename = '/content/drive/My Drive/Applicability domains/Similarities/Similarity indicator/Machine learning/'\n",
        "        filename = filename + str(checkpoint_target) + '/Models vs other targets/'    \n",
        "        filename = filename + '- All results/' + 'all_df_' + 'NEW_multialldata_'\n",
        "        filename = filename + str(nBits) + 'bits_' + str(T_sim_threshold) + 'T_threshold_mol_sim_indicator.csv'\n",
        "\n",
        "        all_df = pd.read_csv(filename)\n",
        "        all_df = all_df.drop(['Unnamed: 0'],axis=1)\n",
        "\n",
        "        # Drop last row since might be incomplete\n",
        "        all_df = all_df.iloc[:-1]\n",
        "\n",
        "        initial = len(all_df) + 1\n",
        "\n",
        "\n",
        "    if state == 'No':\n",
        "        all_df = 0\n",
        "        initial = 0\n",
        "\n",
        "    return initial,all_df\n",
        "\n",
        "#===================================================================================================#\n",
        "\n",
        "# For checkpoint\n",
        "# If not starting from checkpoint, change state to 'No'\n",
        "\n",
        "initial, all_df = Checkpoint(checkpoint_target,state)\n",
        "print('\\nCheckpoint function return parameters: Initial, All_df')\n",
        "print(initial,'\\n', all_df)\n",
        "\n",
        "# Start main loop\n",
        "for train_target in train_target_ls:\n",
        "    sim_ls = []\n",
        "    sim_ls.clear()\n",
        "\n",
        "    mol_threshold_ls = []\n",
        "    mol_threshold_ls.clear()\n",
        "\n",
        "    temp_ls = []\n",
        "    temp_ls.clear()\n",
        "    if isinstance(all_df, pd.DataFrame) == False:\n",
        "        target_count = 0\n",
        "    else:\n",
        "        target_count = 1     \n",
        "\n",
        "    # Read file 1 (training dataset)\n",
        "    print ('\\nReading file 1...')\n",
        "    print ('Current time:')\n",
        "    print(strftime(\"%H:%M:%S\", gmtime())) \n",
        "    start = time.time()\n",
        "\n",
        "    # Create folder in google drive\n",
        "    root_dir = '/content/drive/My Drive/Applicability domains/Similarities/Similarity indicator/Machine learning/'\n",
        "    root_dir = root_dir + str(train_target) + '/Models vs other targets/'\n",
        "        \n",
        "    filename = '/content/drive/My Drive/Applicability domains/Similarities/Similarity indicator/Machine learning/'\n",
        "    filename = filename + str(train_target) + '/'\n",
        "    filename = filename + str(train_target) + '_train_fingerprints ECFP4 10000.csv'\n",
        "    train_fp_df = pd.read_csv(filename)\n",
        "\n",
        "    train_fp_df = train_fp_df.sample(frac=1)\n",
        "    train_fp_df = train_fp_df.reset_index(drop=True)\n",
        "\n",
        "    #print (train_fp_df)\n",
        "    print (train_fp_df.shape)\n",
        "\n",
        "    # Split training data into actives and inactives\n",
        "    print('\\nSPLITTING TRAINING DATA...')\n",
        "    train_active, train_inactive = active_inactive_split(train_fp_df)\n",
        "    train_active_fp, train_active_label = feature_label_split(train_active)\n",
        "    train_inactive_fp, train_inactive_label = feature_label_split(train_inactive)\n",
        "    print('\\nTRAINING DATA SPLIT')\n",
        "\n",
        "    print('\\nCREATING BITVECT DFs FOR TRAINING DATA...')\n",
        "    train_active_bv_df = CreateBitVect(train_active_fp)\n",
        "    train_inactive_bv_df = CreateBitVect(train_inactive_fp)\n",
        "    print('\\nTRAINING BITVECT DFs CREATED')\n",
        "\n",
        "    end = time.time()\n",
        "    elapsed = end - start\n",
        "    minutes = elapsed // 60\n",
        "    seconds = elapsed - (minutes*60)\n",
        "    print('File 1 took {} minutes and {} seconds to read'.format(minutes,seconds))\n",
        "\n",
        "    for protein in range(initial,len(target_df)):\n",
        "        \n",
        "        target = str(target_df.loc[protein]['Target'])\n",
        "\n",
        "        print('For train_target: {}'.format(train_target))\n",
        "        print('Processing target: {}'.format(protein))\n",
        "        print('Processing target: {}'.format(target))\n",
        "        \n",
        "        # Create folders\n",
        "        root_dir = '/content/drive/My Drive/Applicability domains/Similarities/Similarity indicator/Machine learning/'\n",
        "        root_dir = root_dir + str(train_target) + '/Models vs other targets/'\n",
        "        foldername = target\n",
        "        if os.path.isdir(root_dir + foldername) == False:\n",
        "            os.mkdir(root_dir + foldername)\n",
        "\n",
        "        if target != train_target:\n",
        "        #if True:\n",
        "\n",
        "            # Read file 2 (test dataset)\n",
        "            print ('\\nReading file 2...')\n",
        "            print ('Current time:')\n",
        "            print(strftime(\"%H:%M:%S\", gmtime())) \n",
        "            start = time.time()\n",
        "                \n",
        "            filename = '/content/drive/My Drive/Applicability domains/Similarities/Similarity indicator/Machine learning/'\n",
        "            filename = filename + str(target) + '/'\n",
        "            filename = filename + str(target) + '_test_fingerprints ECFP4 10000.csv'\n",
        "            test_fp_df = pd.read_csv(filename)\n",
        "\n",
        "            test_fp_df = test_fp_df.sample(frac=1)\n",
        "            test_fp_df = test_fp_df.reset_index(drop=True)\n",
        "\n",
        "            #print (test_fp_df)\n",
        "            print (test_fp_df.shape)\n",
        "\n",
        "            # Split training data into actives and inactives\n",
        "            test_active, test_inactive = active_inactive_split(test_fp_df)\n",
        "            test_active_fp, test_active_label = feature_label_split(test_active)\n",
        "            test_inactive_fp, test_inactive_label = feature_label_split(test_inactive)\n",
        "\n",
        "            end = time.time()\n",
        "            elapsed = end - start\n",
        "            minutes = elapsed // 60\n",
        "            seconds = elapsed - (minutes*60)\n",
        "            print('File 2 took {} minutes and {} seconds to read'.format(minutes,seconds))\n",
        "\n",
        "            #=========================================================================================#\n",
        "\n",
        "            print('\\nCREATING BITVECT DFs FOR TEST DATA...')\n",
        "            test_active_bv_df = CreateBitVect(test_active_fp)\n",
        "            test_inactive_bv_df = CreateBitVect(test_inactive_fp)\n",
        "\n",
        "            print('\\nTEST BITVECT DFs CREATED')\n",
        "            #=========================================================================================#\n",
        "\n",
        "            # Caclulate Tanimoto similarities\n",
        "            # Slow process\n",
        "            print('\\nCALCULATING TANIMOTO SIMILARITIES...')\n",
        "\n",
        "            # Set similarity threshold\n",
        "            T_ls = [0.3]\n",
        "            nBits = 10000\n",
        "\n",
        "            for element in T_ls:\n",
        "                \n",
        "                T_sim_threshold = float(element)\n",
        "                \n",
        "                print('\\nNOW CALCULATING FOR THRESHOLD: {}'.format(element))\n",
        "                print('\\nNOW CALCULATING FOR TARGET: {}'.format(target))\n",
        "                \n",
        "                indicator_ls = []\n",
        "                indicator_ls.clear()\n",
        "\n",
        "                train_count_ls = []\n",
        "                train_count_ls.clear()\n",
        "\n",
        "                test_count_ls = []\n",
        "                test_count_ls.clear()\n",
        "\n",
        "                # Ensure that training bv df is followed by teste bv df for train sim indicator\n",
        "                # Ensure that actives are matched with actives and vice versa\n",
        "                # Start multiprocessing and queues\n",
        "                start = time.time()\n",
        "\n",
        "                queue1 = Queue()\n",
        "                queue2 = Queue() \n",
        "                queue3 = Queue()\n",
        "                queue4 = Queue()\n",
        "\n",
        "                p1 = Process(target= Calc_Similarity, args= (train_active_bv_df,test_active_bv_df,train_active_label,test_active_label,queue1,1))\n",
        "                p2 = Process(target= Calc_Similarity, args= (train_inactive_bv_df,test_inactive_bv_df,train_inactive_label,test_inactive_label,queue2,2))  \n",
        "\n",
        "                p1.start()\n",
        "                p2.start()\n",
        "\n",
        "\n",
        "                p1.join()\n",
        "                print('\\n#=====================================================================#')\n",
        "                print('Process 1 ENDED')\n",
        "                print('\\n#=====================================================================#')\n",
        "                p2.join()\n",
        "                print('\\n#=====================================================================#')\n",
        "                print('Process 2 ENDED')\n",
        "                print('\\n#=====================================================================#')\n",
        "\n",
        "                # Get return values via queues for training with test\n",
        "                temp_count1 = queue1.get()\n",
        "                temp_count2 = queue2.get()\n",
        "\n",
        "                # Get sim_count for training with test\n",
        "                sim_count = temp_count1 + temp_count2\n",
        "                p1.terminate()\n",
        "                p2.terminate()\n",
        "                queue1.put('exit')\n",
        "                queue2.put('exit')\n",
        "\n",
        "                p3 = Process(target= Calc_Similarity, args= (test_active_bv_df,train_active_bv_df,test_active_label,train_active_label,queue3,3))\n",
        "                p4 = Process(target= Calc_Similarity, args= (test_inactive_bv_df,train_inactive_bv_df,test_inactive_label,train_inactive_label,queue4,4))  \n",
        "                p3.start()\n",
        "                p4.start()\n",
        "\n",
        "                p3.join()\n",
        "                print('\\n#=====================================================================#')\n",
        "                print('Process 3 ENDED')\n",
        "                print('\\n#=====================================================================#')\n",
        "                p4.join()\n",
        "                print('\\n#=====================================================================#')\n",
        "                print('Process 4 ENDED')\n",
        "                print('\\n#=====================================================================#')\n",
        "\n",
        "                # Get return values via queues for test with training\n",
        "                temp_count3 = queue3.get()\n",
        "                temp_count4 = queue4.get()\n",
        "\n",
        "                # Get sim_count for test with training\n",
        "                sim_count2 = temp_count3 + temp_count4\n",
        "                p3.terminate()\n",
        "                p4.terminate()\n",
        "                queue3.put('exit')\n",
        "                queue4.put('exit') \n",
        "\n",
        "                end = time.time()\n",
        "                elapsed = end - start\n",
        "                minutes = elapsed // 60\n",
        "                seconds = elapsed - (minutes*60)\n",
        "\n",
        "                print('\\n#=====================================================================#')\n",
        "                print('\\n#=====================================================================#')\n",
        "                print('\\nTRAINING AND TEST TANIMOTO SIMILARITIES CALCULATED')\n",
        "                print('Code took {} minutes and {} seconds'.format(minutes,seconds))\n",
        "                print('\\n#=====================================================================#')\n",
        "                print('\\n#=====================================================================#')\n",
        "\n",
        "                #=========================================================================================#\n",
        "                print('\\nCALCULATING TRAINING SIMILARITY INDICATOR...')\n",
        "\n",
        "                train_indicator = sim_count / (len(train_active_bv_df)+len(train_inactive_bv_df)) * 100\n",
        "                indicator_ls.append(train_indicator)\n",
        "\n",
        "                print('\\nTRAINING SIMILARITY INDICATOR CALCULATED')\n",
        "                #=========================================================================================#\n",
        "\n",
        "                print('\\nCALCULATING TEST SIMILARITY INDICATOR...')\n",
        "\n",
        "                test_indicator =  sim_count2 / (len(test_active_bv_df)+len(test_inactive_bv_df)) * 100\n",
        "                indicator_ls.append(test_indicator)\n",
        "\n",
        "                print('\\nTEST SIMILARITY INDICATOR CALCULATED')\n",
        "\n",
        "                #=========================================================================================#\n",
        "                print('\\nCALCULATING AVERAGE SIMILARITY INDICATOR...')\n",
        "\n",
        "                average_indicator = (train_indicator + test_indicator) / 2\n",
        "                indicator_ls.append(average_indicator)\n",
        "                \n",
        "                indicator_df = pd.DataFrame(indicator_ls).T\n",
        "                indicator_df.columns = ['Train','Test','Average']\n",
        "\n",
        "                print(indicator_df)\n",
        "\n",
        "                print('\\nAVERAGE SIMILARITY INDICATOR CALCULATED')\n",
        "\n",
        "                #=========================================================================================#\n",
        "\n",
        "                print ('\\nSAVING FILES...')\n",
        "                start = time.time()\n",
        "                print ('Current time:')\n",
        "                print(strftime(\"%H:%M:%S\", gmtime()))\n",
        "                root_dir = '/content/drive/My Drive/Applicability domains/Similarities/Similarity indicator/Machine learning/'\n",
        "                root_dir = root_dir + str(train_target) + '/Models vs other targets/'\n",
        "\n",
        "                filename = root_dir\n",
        "                filename = filename  + str(target) + '/'\n",
        "                filename = filename + str(target) + '_' + 'NEW_multialldata_'\n",
        "                filename = filename + str(nBits) + 'bits_' + str(T_sim_threshold) + 'T_threshold_mol_sim_indicator.csv'\n",
        "                indicator_df.to_csv(filename)\n",
        "\n",
        "                end = time.time()\n",
        "                elapsed = end - start\n",
        "                minutes = elapsed // 60\n",
        "                seconds = elapsed - (minutes*60)\n",
        "                print('Files took {} minutes and {} seconds to save'.format(minutes,seconds))\n",
        "                print ('\\n#=========================================================================#')\n",
        "\n",
        "                # Append data counts in dfs\n",
        "                train_count_ls.append((len(train_active_bv_df)+len(train_inactive_bv_df)))\n",
        "                test_count_ls.append((len(test_active_bv_df)+len(test_inactive_bv_df)))\n",
        "\n",
        "            #=========================================================================================#\n",
        "            # Collate all files/results\n",
        "            # Quick process\n",
        "            print('\\nCOLLATING ALL FILES FOR TARGET {}...'.format(target))\n",
        "            f_count = 0\n",
        "\n",
        "            sim_ls = []\n",
        "            sim_ls.clear()\n",
        "\n",
        "            temp_ls = []\n",
        "            temp_ls.clear()\n",
        "\n",
        "            for element in T_ls:\n",
        "                T_sim_threshold = float(element)\n",
        "              \n",
        "                # Read file \n",
        "                print ('\\nReading file ...')\n",
        "                print ('Current time:')\n",
        "                print(strftime(\"%H:%M:%S\", gmtime())) \n",
        "                start = time.time()\n",
        "                root_dir = '/content/drive/My Drive/Applicability domains/Similarities/Similarity indicator/Machine learning/'\n",
        "                root_dir = root_dir + str(train_target) + '/Models vs other targets/'\n",
        "\n",
        "                filename = root_dir + str(target) + '/'\n",
        "                filename = filename + str(target) + '_' + 'NEW_multialldata_'\n",
        "                filename = filename + str(nBits) + 'bits_' + str(T_sim_threshold) + 'T_threshold_mol_sim_indicator.csv'\n",
        "                file1 = pd.read_csv(filename)\n",
        "                file1mod = file1.drop(['Unnamed: 0'], axis=1)\n",
        "\n",
        "                end = time.time()\n",
        "                elapsed = end - start\n",
        "                minutes = elapsed // 60\n",
        "                seconds = elapsed - (minutes*60)\n",
        "                print('File 1 took {} minutes and {} seconds to read'.format(minutes,seconds))\n",
        "\n",
        "                if f_count == 0:\n",
        "                    combined_df = file1mod\n",
        "                    f_count += 1\n",
        "\n",
        "                else:\n",
        "                    combined_df = pd.concat([combined_df,file1mod],axis = 0)\n",
        "                    \n",
        "                sim_ls.append(T_sim_threshold)\n",
        "                temp_ls.append(target)\n",
        "\n",
        "            combined_df['Sim'] = sim_ls\n",
        "            combined_df['Target'] = temp_ls\n",
        "            combined_df['train_target count'] = train_count_ls\n",
        "            combined_df['test_target count'] = test_count_ls\n",
        "            print(combined_df)\n",
        "\n",
        "            print('\\nALL FILES COLLATED FOR TARGET {}'.format(target))\n",
        "            #===========================================================================#\n",
        "            # Process df and save\n",
        "\n",
        "            print('\\nSAVING DF FOR TARGET {}'.format(protein))\n",
        "            print ('\\nSAVING FILES...')\n",
        "            start = time.time()\n",
        "            print ('Current time:')\n",
        "            print(strftime(\"%H:%M:%S\", gmtime()))\n",
        "\n",
        "            # Create folder in google drive\n",
        "            root_dir = '/content/drive/My Drive/Applicability domains/Similarities/Similarity indicator/Machine learning/'\n",
        "            root_dir = root_dir + str(train_target) + '/Models vs other targets/'\n",
        "\n",
        "            # Create folders \n",
        "            foldername = str(target) + '/Combined'\n",
        "            if os.path.isdir(root_dir + foldername) == False:\n",
        "                os.mkdir(root_dir + foldername)\n",
        "\n",
        "            filename = root_dir + str(target) + '/Combined/'\n",
        "            filename = filename + str(target) + '_Combined_' + 'NEW_multialldata_'\n",
        "            filename = filename + str(nBits) + 'bits_' + str(T_sim_threshold) + 'T_threshold_mol_sim_indicator.csv'\n",
        "            combined_df.to_csv(filename)\n",
        "            \n",
        "            if target_count == 0:\n",
        "                all_df = combined_df\n",
        "                target_count += 1\n",
        "            else:\n",
        "                all_df = pd.concat([all_df,combined_df],axis=0)\n",
        "\n",
        "            end = time.time()\n",
        "            elapsed = end - start\n",
        "            minutes = elapsed // 60\n",
        "            seconds = elapsed - (minutes*60)\n",
        "            print('Files took {} minutes and {} seconds to save'.format(minutes,seconds))\n",
        "            print ('\\n#=========================================================================#')\n",
        "\n",
        "            #===========================================================================================================================#\n",
        "            # Save all_df in separate easy to access folder (combined df with all calculated targets for easy copying)\n",
        "            print('\\nSAVING ALL RESULTS FILE...')\n",
        "            filename = '/content/drive/My Drive/Applicability domains/Similarities/Similarity indicator/Machine learning/'\n",
        "            filename = filename + str(train_target) + '/Models vs other targets/'    \n",
        "            filename = filename + '- All results/' + 'all_df_' + 'NEW_multialldata_'\n",
        "            filename = filename + str(nBits) + 'bits_' + str(T_sim_threshold) + 'T_threshold_mol_sim_indicator.csv'\n",
        "            all_df.to_csv(filename)\n",
        "            print('\\nALL RESULTS FILE SAVED')\n",
        "\n",
        "    target_count = 0\n",
        "\n",
        "print('\\nFINISHED')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVqvMn_M2HyT"
      },
      "source": [
        "# For scrubbing datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_F8ap82qDDM"
      },
      "source": [
        "\"\"\"\n",
        "Created Jun 2021\n",
        "\n",
        "Author: Marcus Wei How Wang\n",
        "Code available at: https://github.com/Goodman-lab/AD-transferability\n",
        "Please acknowledge the authors if using the code, whether partially or in full\n",
        "\"\"\"\n",
        "# For use with model vs other targets from Tim's code\n",
        "# LATEST CODE UPDATED 20 MAR 2021\n",
        "# Code for calculating applicability domain metric based on Tanimoto similarity\n",
        "\n",
        "%%time\n",
        "# Note google colab disconnects and clears data after 12 hrs of inactivity\n",
        "# But code still runs in the background even if runtime is disconnected before the 12hr mark\n",
        "# !pip install -I tensorflow\n",
        "# !pip install -I keras\n",
        "# Relevant imports\n",
        "import numpy as np\n",
        "import pandas as pd # uses pandas python module to view and analyse data\n",
        "\n",
        "import time\n",
        "from time import strftime, gmtime\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import itertools \n",
        "\n",
        "import os\n",
        "from os import mkdir\n",
        "\n",
        "from rdkit import Chem\n",
        "from rdkit import DataStructs\n",
        "\n",
        "from rdkit.Chem import AllChem\n",
        "from rdkit.Chem import Draw\n",
        "from rdkit.Chem import rdMolDescriptors\n",
        "from rdkit.Chem import PandasTools\n",
        "from rdkit.Chem.Draw.MolDrawing import MolDrawing\n",
        "from rdkit.Chem.Draw import IPythonConsole\n",
        "from rdkit.Chem import MACCSkeys\n",
        "from rdkit.Chem.rdMolDescriptors import GetMACCSKeysFingerprint\n",
        "\n",
        "#=======================================================================================#\n",
        "# READ REQUIRED FILES\n",
        "# Files contain the SMILES string\n",
        "\n",
        "# Read csv file containing targets to calculate\n",
        "filename = '/content/drive/My Drive/Applicability domains/Similarities/Similarity indicator/Machine learning/'\n",
        "#filename = filename + 'Targets to calculate 5.csv'\n",
        "filename = filename + 'Targets for optimisation.csv'\n",
        "\n",
        "target_df = pd.read_csv(filename)\n",
        "\n",
        "target_df = target_df[['Target']]\n",
        "print(target_df)\n",
        "\n",
        "error_ls = []\n",
        "error_ls.clear()\n",
        "\n",
        "train_target_ls = ['AChE','ADORA2A','AR','hERG','SERT']\n",
        "#train_target_ls = ['ADORA2A','AR','hERG','SERT']\n",
        "#train_target_ls = ['AChE']\n",
        "#train_target_ls = ['AChE','ADORA2A']\n",
        "#train_target_ls = ['AR','hERG']\n",
        "#train_target_ls = ['hERG','SERT']\n",
        "#train_target_ls = ['SERT']\n",
        "\n",
        "\n",
        "\n",
        "for train_target in train_target_ls:\n",
        "    sim_ls = []\n",
        "    sim_ls.clear()\n",
        "\n",
        "    mol_threshold_ls = []\n",
        "    mol_threshold_ls.clear()\n",
        "\n",
        "    temp_ls = []\n",
        "    temp_ls.clear()\n",
        "    target_count = 0\n",
        "\n",
        "    # Read file 1 (training dataset)\n",
        "    print ('\\nReading file 1...')\n",
        "    print ('Current time:')\n",
        "    print(strftime(\"%H:%M:%S\", gmtime())) \n",
        "    start = time.time()\n",
        "        \n",
        "    filename = '/content/drive/My Drive/Applicability domains/Similarities/Similarity indicator/Machine learning/'\n",
        "    filename = filename + str(train_target) + '/'\n",
        "    filename = filename + str(train_target) + '_train_fingerprints ECFP4 10000.csv'\n",
        "    train_fp_df = pd.read_csv(filename)\n",
        "\n",
        "    train_fp_df = train_fp_df.sample(frac=1)\n",
        "    train_fp_df = train_fp_df.reset_index(drop=True)\n",
        "\n",
        "    #print (train_fp_df)\n",
        "    print (train_fp_df.shape)\n",
        "\n",
        "    train_fp = train_fp_df.iloc[:,0:10000]\n",
        "    train_fp['combined'] = train_fp[train_fp.columns.tolist()].apply(lambda row: ''.join(row.values.astype(str)), axis=1)\n",
        "    train_fp = train_fp[['combined']]\n",
        "    train_fp_active = train_fp_df.iloc[:,-1:]\n",
        "\n",
        "    end = time.time()\n",
        "    elapsed = end - start\n",
        "    minutes = elapsed // 60\n",
        "    seconds = elapsed - (minutes*60)\n",
        "    print('File 1 took {} minutes and {} seconds to read'.format(minutes,seconds))\n",
        "\n",
        "    for protein in range(0,len(target_df)):\n",
        "        \n",
        "        target = str(target_df.loc[protein]['Target'])\n",
        "        print('For train_target: {}'.format(train_target))\n",
        "        print('Processing target: {}'.format(protein))\n",
        "        print('Processing target: {}'.format(target))\n",
        "        \n",
        "        # Create folders\n",
        "        root_dir = '/content/drive/My Drive/Applicability domains/Similarities/Similarity indicator/Machine learning/- Scrubbing datasets/'\n",
        "        root_dir = root_dir + str(train_target) + '/'\n",
        "        foldername = target\n",
        "        if os.path.isdir(root_dir + foldername) == False:\n",
        "            os.mkdir(root_dir + foldername)\n",
        "\n",
        "        #if target != train_target:\n",
        "        if True:\n",
        "\n",
        "            # Read file 2 (test dataset)\n",
        "            print ('\\nReading file 2...')\n",
        "            print ('Current time:')\n",
        "            print(strftime(\"%H:%M:%S\", gmtime())) \n",
        "            start = time.time()\n",
        "                \n",
        "            filename = '/content/drive/My Drive/Applicability domains/Similarities/Similarity indicator/Machine learning/'\n",
        "            filename = filename + str(target) + '/'\n",
        "            filename = filename + str(target) + '_test_fingerprints ECFP4 10000.csv'\n",
        "            test_fp_df = pd.read_csv(filename)\n",
        "\n",
        "            test_fp_df = test_fp_df.sample(frac=1)\n",
        "            test_fp_df = test_fp_df.reset_index(drop=True)\n",
        "\n",
        "            #print (test_fp_df)\n",
        "            print (test_fp_df.shape)\n",
        "\n",
        "            test_fp = test_fp_df.iloc[:,0:10000]\n",
        "            test_fp['combined'] = test_fp[test_fp.columns.tolist()].apply(lambda row: ''.join(row.values.astype(str)), axis=1)\n",
        "            test_fp = test_fp[['combined']]\n",
        "            test_fp_active = test_fp_df.iloc[:,-1:]\n",
        "\n",
        "            end = time.time()\n",
        "            elapsed = end - start\n",
        "            minutes = elapsed // 60\n",
        "            seconds = elapsed - (minutes*60)\n",
        "            print('File 2 took {} minutes and {} seconds to read'.format(minutes,seconds))\n",
        "\n",
        "            #=========================================================================================#\n",
        "\n",
        "            start = time.time()\n",
        "            print('\\nPROCESSING AND SAMPLING DATA FROM BOTH FILES...')\n",
        "\n",
        "            # IF SAMPLING\n",
        "            # Get count of data points for both datasets\n",
        "            # sample_size = 4000\n",
        "            # training_sample_fp = train_fp[0:sample_size]\n",
        "            # training_sample_active = train_fp_active[0:sample_size]\n",
        "            # test_sample_fp = test_fp[0:sample_size]\n",
        "            # test_sample_active = test_fp_active[0:sample_size]\n",
        "\n",
        "            # IF NOT SAMPLING\n",
        "            # Get count of data points for both datasets\n",
        "            sample_size = 'all'\n",
        "            training_sample_fp = train_fp\n",
        "            training_sample_active = train_fp_active\n",
        "            test_sample_fp = test_fp\n",
        "            test_sample_active = test_fp_active\n",
        "\n",
        "            train_count_ls = []\n",
        "            test_count_ls = []\n",
        "\n",
        "            print('\\nFILES PROCESSED AND SAMPLED')\n",
        "            #=========================================================================================#\n",
        "            start = time.time()\n",
        "            print('\\nCREATING BITVECT DFs FOR BOTH TRAINING AND TEST SAMPLES...')\n",
        "            train_bit_ls = []\n",
        "            test_bit_ls = []\n",
        "\n",
        "            train_bit_ls.clear()\n",
        "            test_bit_ls.clear()\n",
        "\n",
        "            for x in range(0,len(training_sample_fp)):\n",
        "                training_bitvect = DataStructs.CreateFromBitString(training_sample_fp.iloc[x]['combined'])\n",
        "                train_bit_ls.append(training_bitvect)\n",
        "            for y in range(0,len(test_sample_fp)):\n",
        "                test_bitvect = DataStructs.CreateFromBitString(test_sample_fp.iloc[y]['combined'])\n",
        "                test_bit_ls.append(test_bitvect)\n",
        "\n",
        "            train_bv_df = pd.DataFrame(train_bit_ls,columns = ['BV'])\n",
        "            test_bv_df = pd.DataFrame(test_bit_ls,columns = ['BV'])\n",
        "\n",
        "            print('\\nBITVECT DFs CREATED')\n",
        "            #=========================================================================================#\n",
        "\n",
        "            print('\\nCALCULATING TRAINING TANIMOTO SIMILARITIES...')\n",
        "\n",
        "            # Set similarity thresholds and no. of similar molecules threshold\n",
        "            T_ls = [0.2]\n",
        "            mol_ls = [1]\n",
        "            nBits = 10000\n",
        "\n",
        "            for element in T_ls:\n",
        "                \n",
        "                T_sim_threshold = float(element)\n",
        "\n",
        "                for ele in mol_ls:\n",
        "                    print('\\nNOW CALCULATING FOR THRESHOLD: {}'.format(element))\n",
        "                    print('\\nNOW CALCULATING FOR nMOL: {}'.format(ele))\n",
        "                    print('\\nNOW CALCULATING FOR TARGET: {}'.format(target))\n",
        "\n",
        "                    sim_count = 0\n",
        "                    indicator_ls = []\n",
        "                    indicator_ls.clear()\n",
        "\n",
        "                    train_extract_index = []\n",
        "                    train_extract_index.clear()\n",
        "\n",
        "                    mol_threshold = int(ele)\n",
        "\n",
        "                    # Calculate Tanimoto similarity between both samples\n",
        "                    for x in range(0,len(train_bv_df)):\n",
        "                        train_bv = train_bv_df.iloc[x]['BV']\n",
        "                        training_active_state = training_sample_active.iloc[x]['Binary Activity']\n",
        "                        if x % 100 == 0:\n",
        "                            print ('Current time:')\n",
        "                            print(strftime(\"%H:%M:%S\", gmtime()))\n",
        "                            print('NOW CALCULATING SIMILARITIES FOR TRAINING INDEX {}'.format(x))\n",
        "\n",
        "\n",
        "                        for y in range(0,len(test_bv_df)):\n",
        "                            temp_count = 0\n",
        "                            test_bv = test_bv_df.iloc[y]['BV']\n",
        "                            sim = DataStructs.TanimotoSimilarity(train_bv,test_bv)\n",
        "                            test_active_state = test_sample_active.iloc[y]['Binary Activity']\n",
        "\n",
        "                            # Process sim per molecule and determine if molecule is similar to second dataset                            \n",
        "                            if sim >= T_sim_threshold:\n",
        "                                if test_active_state == training_active_state:\n",
        "                                    temp_count += 1\n",
        "                        \n",
        "                                    if temp_count >= mol_threshold:\n",
        "                                        sim_count += 1\n",
        "                                        train_extract_index.append(x)\n",
        "                                        break\n",
        "                          \n",
        "                    print('\\nTRAINING TANIMOTO SIMILARITIES CALCULATED')\n",
        "\n",
        "                    #=========================================================================================#\n",
        "                    print('\\nCREATING TRAIN EXTRACT_DF...')\n",
        "\n",
        "\n",
        "                    train_extract_df = train_fp_df.iloc[train_extract_index]\n",
        "\n",
        "                    root_dir = '/content/drive/My Drive/Applicability domains/Similarities/Similarity indicator/Machine learning/- Scrubbing datasets/scrubbed data/'\n",
        "                    filename = root_dir  + str(train_target) + '/'\n",
        "                    filename = filename + str(train_target) + '_' + 'training_data_' + str(target) + 'test_target_'\n",
        "                    filename = filename + str(nBits) + 'bits_' + str(T_sim_threshold) + 'T_threshold_' + str(mol_threshold) + 'mol_sim_indicator.csv'\n",
        "                    train_extract_df.to_csv(filename)\n",
        "\n",
        "                    print('\\nTRAIN EXTRACT_DF CREATED')\n",
        "\n",
        "                    #=========================================================================================#\n",
        "                    print('\\nCALCULATING TRAINING SIMILARITY INDICATOR...')\n",
        "\n",
        "                    train_indicator = sim_count / len(train_bv_df) * 100\n",
        "                    indicator_ls.append(train_indicator)\n",
        "\n",
        "                    print('\\nTRAINING SIMILARITY INDICATOR CALCULATED')\n",
        "                    #=========================================================================================#\n",
        "\n",
        "                    print('\\nCALCULATING TEST TANIMOTO SIMILARITIES...')\n",
        "                    \n",
        "                    # Calculate Tanimoto similarity between both samples\n",
        "                    sim_count = 0\n",
        "                    test_extract_index = []\n",
        "                    test_extract_index.clear()\n",
        "\n",
        "                    for x in range(0,len(test_bv_df)):\n",
        "                        test_bv = test_bv_df.iloc[x]['BV']\n",
        "                        test_active_state = test_sample_active.iloc[x]['Binary Activity']\n",
        "                        if x % 100 == 0:\n",
        "                            print ('Current time:')\n",
        "                            print(strftime(\"%H:%M:%S\", gmtime()))\n",
        "                            print('NOW CALCULATING SIMILARITIES FOR TRAINING INDEX {}'.format(x))\n",
        "\n",
        "\n",
        "                        for y in range(0,len(train_bv_df)):\n",
        "                            temp_count = 0\n",
        "                            train_bv = train_bv_df.iloc[y]['BV']\n",
        "                            sim = DataStructs.TanimotoSimilarity(train_bv,test_bv)\n",
        "                            training_active_state = training_sample_active.iloc[y]['Binary Activity']\n",
        "\n",
        "                            # Process sim per molecule and determine if molecule is similar to second dataset                            \n",
        "                            if sim >= T_sim_threshold:\n",
        "                                if test_active_state == training_active_state:\n",
        "                                    temp_count += 1\n",
        "                        \n",
        "                                    if temp_count >= mol_threshold:\n",
        "                                        sim_count += 1\n",
        "                                        test_extract_index.append(x)\n",
        "                                        break\n",
        "                          \n",
        "                    print('\\nTEST TANIMOTO SIMILARITIES CALCULATED')\n",
        "\n",
        "                    #=========================================================================================#\n",
        "                    print('\\nCREATING TEST EXTRACT_DF...')\n",
        "\n",
        "                    test_extract_df = test_fp_df.iloc[test_extract_index]\n",
        "\n",
        "                    root_dir = '/content/drive/My Drive/Applicability domains/Similarities/Similarity indicator/Machine learning/- Scrubbing datasets/scrubbed data/'\n",
        "                    filename = root_dir  + str(train_target) + '/'\n",
        "                    filename = filename + str(train_target) + '_' + 'test_data_' + str(target) + 'test_target_'\n",
        "                    filename = filename + str(nBits) + 'bits_' + str(T_sim_threshold) + 'T_threshold_' + str(mol_threshold) + 'mol_sim_indicator.csv'\n",
        "                    test_extract_df.to_csv(filename)\n",
        "\n",
        "                    print('\\nTEST EXTRACT_DF CREATED')\n",
        "\n",
        "                    #=========================================================================================#\n",
        "                    print('\\nCALCULATING TEST SIMILARITY INDICATOR...')\n",
        "\n",
        "                    test_indicator = sim_count / len(test_bv_df) * 100\n",
        "                    indicator_ls.append(test_indicator)\n",
        "\n",
        "                    print('\\nTEST SIMILARITY INDICATOR CALCULATED')\n",
        "\n",
        "                    #=========================================================================================#\n",
        "                    print('\\nCALCULATING AVERAGE SIMILARITY INDICATOR...')\n",
        "\n",
        "                    average_indicator = (train_indicator + test_indicator) / 2\n",
        "                    indicator_ls.append(average_indicator)\n",
        "                    \n",
        "\n",
        "                    indicator_df = pd.DataFrame(indicator_ls).T\n",
        "                    indicator_df.columns = ['Train','Test','Average']\n",
        "\n",
        "                    print(indicator_df)\n",
        "\n",
        "                    print('\\nAVERAGE SIMILARITY INDICATOR CALCULATED')\n",
        "\n",
        "                    #=========================================================================================#\n",
        "\n",
        "                    print ('\\nSAVING FILES...')\n",
        "                    start = time.time()\n",
        "                    print ('Current time:')\n",
        "                    print(strftime(\"%H:%M:%S\", gmtime()))\n",
        "\n",
        "                    root_dir = '/content/drive/My Drive/Applicability domains/Similarities/Similarity indicator/Machine learning/- Scrubbing datasets/'\n",
        "                    root_dir = root_dir + str(train_target) + '/'\n",
        "\n",
        "                    filename = root_dir  + str(target) + '/'\n",
        "                    filename = filename + str(target) + '_' + 'NEW_' + str(sample_size) + 'data_'\n",
        "                    filename = filename + str(nBits) + 'bits_' + str(T_sim_threshold) + 'T_threshold_' + str(mol_threshold) + 'mol_sim_indicator.csv'\n",
        "                    indicator_df.to_csv(filename)\n",
        "\n",
        "                    end = time.time()\n",
        "                    elapsed = end - start\n",
        "                    minutes = elapsed // 60\n",
        "                    seconds = elapsed - (minutes*60)\n",
        "                    print('Files took {} minutes and {} seconds to save'.format(minutes,seconds))\n",
        "                    print ('\\n#=========================================================================#')\n",
        "\n",
        "                    train_count_ls.append(len(training_sample_fp))\n",
        "                    test_count_ls.append(len(test_sample_fp))\n",
        "\n",
        "            # Collate all files/results\n",
        "            print('\\nCOLLATING ALL FILES...')\n",
        "            f_count = 0\n",
        "\n",
        "            sim_ls = []\n",
        "            sim_ls.clear()\n",
        "\n",
        "            mol_threshold_ls = []\n",
        "            mol_threshold_ls.clear()\n",
        "\n",
        "            temp_ls = []\n",
        "            temp_ls.clear()\n",
        "\n",
        "            for element in T_ls:\n",
        "                T_sim_threshold = float(element)\n",
        "\n",
        "                for ele in mol_ls:\n",
        "                    mol_threshold = int(ele)\n",
        "                  \n",
        "                    # Read file \n",
        "                    print ('\\nReading file ...')\n",
        "                    print ('Current time:')\n",
        "                    print(strftime(\"%H:%M:%S\", gmtime())) \n",
        "                    start = time.time()\n",
        "\n",
        "                    root_dir = '/content/drive/My Drive/Applicability domains/Similarities/Similarity indicator/Machine learning/- Scrubbing datasets/'\n",
        "                    root_dir = root_dir + str(train_target) + '/'\n",
        "\n",
        "                    filename = root_dir + str(target) + '/'\n",
        "                    filename = filename + str(target) + '_' + 'NEW_' + str(sample_size) + 'data_'\n",
        "                    filename = filename + str(nBits) + 'bits_' + str(T_sim_threshold) + 'T_threshold_' + str(mol_threshold) + 'mol_sim_indicator.csv'\n",
        "                    file1 = pd.read_csv(filename)\n",
        "                    file1mod = file1.drop(['Unnamed: 0'], axis=1)\n",
        "\n",
        "                    end = time.time()\n",
        "                    elapsed = end - start\n",
        "                    minutes = elapsed // 60\n",
        "                    seconds = elapsed - (minutes*60)\n",
        "                    print('File 1 took {} minutes and {} seconds to read'.format(minutes,seconds))\n",
        "\n",
        "                    if f_count == 0:\n",
        "                        combined_df = file1mod\n",
        "                        f_count += 1\n",
        "\n",
        "                    else:\n",
        "                        combined_df = pd.concat([combined_df,file1mod],axis = 0)\n",
        "                        \n",
        "                    sim_ls.append(T_sim_threshold)\n",
        "                    mol_threshold_ls.append(mol_threshold)\n",
        "                    temp_ls.append(target)\n",
        "\n",
        "            print('\\nALL FILES COLLATED')\n",
        "            #===========================================================================#\n",
        "            # Process df and save\n",
        "\n",
        "            print('\\nSAVING DF FOR TARGET {}'.format(protein))\n",
        "            combined_df['Sim'] = sim_ls\n",
        "            combined_df['No. of mol'] = mol_threshold_ls\n",
        "            combined_df['Target'] = temp_ls\n",
        "            combined_df['train_target count'] = train_count_ls\n",
        "            combined_df['test_target count'] = test_count_ls\n",
        "\n",
        "            print(combined_df)\n",
        "\n",
        "            print ('\\nSAVING FILES...')\n",
        "            start = time.time()\n",
        "            print ('Current time:')\n",
        "            print(strftime(\"%H:%M:%S\", gmtime()))\n",
        "\n",
        "            # Create folder in google drive\n",
        "            root_dir = '/content/drive/My Drive/Applicability domains/Similarities/Similarity indicator/Machine learning/- Scrubbing datasets/'\n",
        "            root_dir = root_dir + str(train_target) + '/'\n",
        "\n",
        "            # Create folders \n",
        "            foldername = str(target) + '/Combined'\n",
        "            if os.path.isdir(root_dir + foldername) == False:\n",
        "                os.mkdir(root_dir + foldername)\n",
        "\n",
        "            filename = root_dir + str(target) + '/Combined/'\n",
        "            filename = filename + str(target) + '_Combined_' + 'NEW_' + str(sample_size) + 'data_'\n",
        "            filename = filename + str(nBits) + 'bits_' + str(T_sim_threshold) + 'T_threshold_' + str(mol_threshold) + 'mol_sim_indicator.csv'\n",
        "            combined_df.to_csv(filename)\n",
        "            \n",
        "            if target_count == 0:\n",
        "\n",
        "                #print('FIRST LINE')\n",
        "                all_df = combined_df\n",
        "                target_count += 1\n",
        "            else:\n",
        "                #print('NEXT LINES')\n",
        "                all_df = pd.concat([all_df,combined_df],axis=0)\n",
        "\n",
        "            end = time.time()\n",
        "            elapsed = end - start\n",
        "            minutes = elapsed // 60\n",
        "            seconds = elapsed - (minutes*60)\n",
        "            print('Files took {} minutes and {} seconds to save'.format(minutes,seconds))\n",
        "            print ('\\n#=========================================================================#')\n",
        "\n",
        "            # Save all_df (combined df with all calculated targets for easy copying)\n",
        "            print('\\nSAVING ALL RESULTS FILE...')\n",
        "            root_dir = '/content/drive/My Drive/Applicability domains/Similarities/Similarity indicator/Machine learning/- Scrubbing datasets/' + str(train_target) + '/'\n",
        "\n",
        "            # Create folders \n",
        "            foldername = '- All results'\n",
        "            if os.path.isdir(root_dir + foldername) == False:\n",
        "                os.mkdir(root_dir + foldername)\n",
        " \n",
        "            filename = root_dir + '- All results/' + 'all_df_' + 'NEW_' + str(sample_size) + 'data_'\n",
        "            filename = filename + str(nBits) + 'bits_' + str(T_sim_threshold) + 'T_threshold_' + str(mol_threshold) + 'mol_sim_indicator.csv'\n",
        "            all_df.to_csv(filename)\n",
        "            print('\\nALL RESULTS FILE SAVED')\n",
        "\n",
        "print('\\nFINISHED')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRlmPvdaUFHV"
      },
      "source": [
        "# -*- ChAIkeras.py -*-\n",
        "\"\"\"\n",
        "Created Oct 2019\n",
        "\n",
        "author: Timothy E H Allen\n",
        "\"\"\"\n",
        "#%%\n",
        "\n",
        "# Import the usual suspects\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras import regularizers\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
        "from sklearn.utils import class_weight\n",
        "import random\n",
        "\n",
        "# DEFINE INPUTS FOR MODEL TRAINING\n",
        "\n",
        "for runs in range(1,26):\n",
        "    # Get test targets\n",
        "    root_dir = '/content/drive/My Drive/Applicability domains/Similarities/Similarity indicator/Machine learning/'\n",
        "    filename = root_dir + 'Targets for optimisation.csv'\n",
        "    receptor_df = pd.read_csv(filename)\n",
        "    receptor_ls = list(receptor_df['Target'])\n",
        "    #receptor_ls = ['AChE']\n",
        "\n",
        "    list1 = []\n",
        "    list2 = []\n",
        "    list3 = []\n",
        "    list4 = []\n",
        "\n",
        "    # Available receptors:\n",
        "    train_receptor = 'AChE'\n",
        "    root_dir = '/content/drive/My Drive/Applicability domains/Similarities/Similarity indicator/Machine learning/'\n",
        "    # filename = root_dir + '- Scrubbing datasets/scrubbed training and test datasets/'\n",
        "    # filename = filename + str(receptor) + 'scrubbedtrainsample_1.csv'\n",
        "\n",
        "    filename = root_dir + '- Scrubbing datasets/raw training and test sets/'\n",
        "    filename = filename + str(train_receptor) + 'trainsample_1.csv'\n",
        "    #print(pd.read_csv(filename))\n",
        "    #input_data_training = \"/content/drive/My Drive/\" + receptor + \"_training_fingerprint.csv\"\n",
        "    input_data_training = filename\n",
        "\n",
        "    # filename = root_dir + str(receptor) + '/'\n",
        "    # filename = filename + str(receptor) + '_test_fingerprints ECFP4 10000.csv'\n",
        "    # input_data_test = filename\n",
        "\n",
        "    rng_1 = random.randrange(1,1000)\n",
        "    rng_2 = random.randrange(1,1000)\n",
        "    validation_proportion = 0.25\n",
        "    beta = 0.1\n",
        "    neurons = 100\n",
        "    hidden_layers = 2\n",
        "    LR = 0.001\n",
        "    epochs = 100\n",
        "\n",
        "\n",
        "    print(\"Welcome to ChAI\")\n",
        "    print(\"Dataset loading...\")\n",
        "\n",
        "    # Reading The Dataset\n",
        "\n",
        "    def read_dataset(input_data):\n",
        "        df = pd.read_csv(input_data)\n",
        "        print (df)\n",
        "        X = df[df.columns[1:10001]].values\n",
        "        # print(df[df.columns[1:10001]])\n",
        "        # print(X)\n",
        "        y = df[df.columns[-1]]\n",
        "        # print(y)\n",
        "        # Encode the dependent variable\n",
        "        encoder = LabelEncoder()\n",
        "        encoder.fit(y)\n",
        "        Y = encoder.transform(y)\n",
        "        #print(\"X.shape =\", X.shape)\n",
        "        #print(\"Y.shape =\", Y.shape)\n",
        "        #print(\"y.shape =\", y.shape)\n",
        "        return (X, Y)\n",
        "\n",
        "    X, Y = read_dataset(input_data_training)\n",
        "    # labels = Y\n",
        "    # positive = 0\n",
        "    # negative = 0\n",
        "    # for x in range(0,len(Y)):\n",
        "    #     check = labels[x]\n",
        "    #     if check == 0:\n",
        "    #         negative += 1\n",
        "    #     if check == 1:\n",
        "    #         positive += 1\n",
        "\n",
        "    # print('\\nTraining dataset stats')\n",
        "    # print('Positives: {}'.format(positive))\n",
        "    # print('Negatives: {}'.format(negative))\n",
        "    # print('Total: {}'.format(len(labels)))\n",
        "    # print('Check Total: {}'.format(positive + negative))\n",
        "\n",
        "    # Shuffle the dataset\n",
        "\n",
        "    X, Y = shuffle(X, Y, random_state=rng_1)\n",
        "\n",
        "    # Convert the dataset into train and validation sets\n",
        "\n",
        "    train_x, valid_x, train_y, valid_y = train_test_split(X, Y, test_size =validation_proportion, random_state=rng_2)\n",
        "\n",
        "    # Inspect the shape of the training and validation data\n",
        "\n",
        "    # print(\"Dimensionality of data:\")\n",
        "    # print(\"Train x shape =\", train_x.shape)\n",
        "    # print(\"Train y shape =\", train_y.shape)\n",
        "    # print(\"Validation x shape =\", valid_x.shape)\n",
        "    # print(\"Validation y shape =\", valid_y.shape)\n",
        "\n",
        "    class_weights = class_weight.compute_class_weight('balanced',\n",
        "                                                    np.unique(train_y),\n",
        "                                                    train_y)\n",
        "\n",
        "    #Define the model in keras\n",
        "\n",
        "    print(\"Constructing model architecture\")\n",
        "\n",
        "    if hidden_layers == 1:\n",
        "        inputs = keras.Input(shape=(10000,), name='digits')\n",
        "        x = layers.Dense(neurons, activation='relu', kernel_initializer='random_uniform', bias_initializer='zeros', kernel_regularizer=regularizers.l2(beta), name='dense_1')(inputs)\n",
        "        outputs = layers.Dense(2, activation='softmax', name='predictions')(x)\n",
        "    elif hidden_layers == 2:\n",
        "        inputs = keras.Input(shape=(10000,), name='digits')\n",
        "        x = layers.Dense(neurons, activation='relu', kernel_initializer='random_uniform', bias_initializer='zeros', kernel_regularizer=regularizers.l2(beta), name='dense_1')(inputs)\n",
        "        x = layers.Dense(neurons, activation='relu', kernel_initializer='random_uniform', bias_initializer='zeros', kernel_regularizer=regularizers.l2(beta), name='dense_2')(x)\n",
        "        outputs = layers.Dense(2, activation='softmax', name='predictions')(x)\n",
        "    elif hidden_layers == 3:\n",
        "        inputs = keras.Input(shape=(10000,), name='digits')\n",
        "        x = layers.Dense(neurons, activation='relu', kernel_initializer='random_uniform', bias_initializer='zeros', kernel_regularizer=regularizers.l2(beta), name='dense_1')(inputs)\n",
        "        x = layers.Dense(neurons, activation='relu', kernel_initializer='random_uniform', bias_initializer='zeros', kernel_regularizer=regularizers.l2(beta), name='dense_2')(x)\n",
        "        x = layers.Dense(neurons, activation='relu', kernel_initializer='random_uniform', bias_initializer='zeros', kernel_regularizer=regularizers.l2(beta), name='dense_3')(x)\n",
        "        outputs = layers.Dense(2, activation='softmax', name='predictions')(x)\n",
        "    else:\n",
        "        print(\"Number of hidden layers outside this model scope, please choose 1, 2 or 3\")\n",
        "\n",
        "    model = keras.Model(inputs = inputs, outputs = outputs)\n",
        "\n",
        "    model.compile(optimizer=keras.optimizers.Adam(lr=LR),\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['sparse_categorical_accuracy'])\n",
        "\n",
        "    print('Commencing model training...')\n",
        "    history = model.fit(train_x, train_y,\n",
        "                        batch_size=128,\n",
        "                        epochs=epochs,\n",
        "                        class_weight=class_weights,\n",
        "                        # We pass some validation for\n",
        "                        # monitoring validation loss and metrics\n",
        "                        # at the end of each epoch\n",
        "                        validation_data=(valid_x, valid_y))\n",
        "\n",
        "    # The returned \"history\" object holds a record\n",
        "    # of the loss values and metric values during training\n",
        "\n",
        "    # Evaluate the model on the training and validation data\n",
        "    #print('\\n# Evaluate on training data')\n",
        "    train_results = model.evaluate(train_x, train_y, batch_size=128)\n",
        "    #print('train loss, train acc:', train_results)\n",
        "\n",
        "    #print('\\n# Evaluate on validation data')\n",
        "    validation_results = model.evaluate(valid_x, valid_y, batch_size=128)\n",
        "    #print('validation loss, validation acc:', validation_results)\n",
        "\n",
        "    # Save the model\n",
        "\n",
        "    count = 0\n",
        "    for receptor in receptor_ls:\n",
        "        model_path = root_dir + '- Scrubbing datasets/Models/' + str(receptor) + 'run' + str(runs) + '_model.h5'\n",
        "        model.save(model_path)\n",
        "        print('Model saved to ' + model_path)\n",
        "        root_dir = '/content/drive/My Drive/Applicability domains/Similarities/Similarity indicator/Machine learning/'\n",
        "        # filename = root_dir + '- Scrubbing datasets/scrubbed training and test datasets/'\n",
        "        # filename = filename + str(receptor) + 'scrubbedtestsample_1.csv'\n",
        "\n",
        "        filename = root_dir + '- Scrubbing datasets/raw training and test sets/'\n",
        "        filename = filename + str(receptor) + 'testsample_1.csv'\n",
        "        #print(pd.read_csv(filename))\n",
        "        input_data_test = filename\n",
        "        test_x, test_y = read_dataset(input_data_test)\n",
        "        # labels = test_y\n",
        "        # positive = 0\n",
        "        # negative = 0\n",
        "        # for x in range(0,len(test_y)):\n",
        "        #     check = labels[x]\n",
        "        #     if check == 0:\n",
        "        #         negative += 1\n",
        "        #     if check == 1:\n",
        "        #         positive += 1\n",
        "\n",
        "        # print('\\nTest dataset stats')\n",
        "        # print('Positives: {}'.format(positive))\n",
        "        # print('Negatives: {}'.format(negative))\n",
        "        # print('Total: {}'.format(len(labels)))\n",
        "        # print('Check Total: {}'.format(positive + negative))\n",
        "\n",
        "        if count == 0:\n",
        "            pred_valid_y = model.predict(valid_x, verbose=1)\n",
        "            pred_train_y = model.predict(train_x, verbose=1)\n",
        "        pred_test_y = model.predict(test_x)\n",
        "\n",
        "        # Define experimental and predicted values using argmax\n",
        "        if count == 0:\n",
        "            pred_train_y_binary = np.argmax(pred_train_y, axis=1)\n",
        "            pred_valid_y_binary = np.argmax(pred_valid_y, axis=1)\n",
        "        pred_test_y_binary = np.argmax(pred_test_y, axis=1)\n",
        "\n",
        "        # Calculate and display confusion matricies\n",
        "        def get_accuracy(cm):\n",
        "            TP = cm[0][0]\n",
        "            FP = cm[0][1]\n",
        "            FN = cm[1][0]\n",
        "            TN = cm[1][1]\n",
        "\n",
        "            accuracy = (TP + TN) / (TP + FP + FN + TN)\n",
        "\n",
        "            return accuracy\n",
        "\n",
        "        def manual_cm(true_y, pred_y):\n",
        "            TP = 0\n",
        "            FP = 0\n",
        "            FN = 0\n",
        "            TN = 0\n",
        "            for z in range(0,len(true_y)):\n",
        "                if true_y[z] == 1 and pred_y[z] == 1:\n",
        "                    TP += 1\n",
        "                if true_y[z] == 0 and pred_y[z] == 1:\n",
        "                    FP += 1\n",
        "                if true_y[z] == 1 and pred_y[z] == 0:\n",
        "                    FN += 1\n",
        "                if true_y[z] == 0 and pred_y[z] == 0:\n",
        "                    TN += 1\n",
        "              \n",
        "            return np.array([[TP,FP],[FN,TN]])    \n",
        "\n",
        "\n",
        "        if count == 0:\n",
        "            cm = confusion_matrix(train_y, pred_train_y_binary)\n",
        "            np.set_printoptions(precision=2)\n",
        "            # print(\"Confusion matrix (Training), without normalisation\")\n",
        "            # print(cm)\n",
        "            train_accuracy = get_accuracy(cm)\n",
        "\n",
        "            cm = confusion_matrix(valid_y, pred_valid_y_binary)\n",
        "            np.set_printoptions(precision=2)\n",
        "            # print(\"Confusion matrix (Validation), without normalisation\")\n",
        "            # print(cm)\n",
        "            valid_accuracy = get_accuracy(cm)\n",
        "\n",
        "            count += 1\n",
        "\n",
        "        cm = confusion_matrix(test_y, pred_test_y_binary)\n",
        "        np.set_printoptions(precision=2)\n",
        "        # print(\"Confusion matrix (Test), without normalisation\")\n",
        "        # print(cm)\n",
        "        test_accuracy = get_accuracy(cm)\n",
        "\n",
        "        # Append all values to lists\n",
        "        list1.append(receptor)\n",
        "        list2.append(train_accuracy)\n",
        "        list3.append(valid_accuracy)\n",
        "        list4.append(test_accuracy)\n",
        "\n",
        "        # Get dataframe of accuracies\n",
        "        accuracy_df = pd.DataFrame(list1,columns = ['Receptor'])\n",
        "        accuracy_df['Train_acc'] = list2\n",
        "        accuracy_df['Valid_acc'] = list3\n",
        "        accuracy_df['Test_acc'] = list4\n",
        "        print(accuracy_df)\n",
        "\n",
        "        root_dir = '/content/drive/My Drive/Applicability domains/Similarities/Similarity indicator/Machine learning/'\n",
        "        filename = root_dir + '/- Scrubbing datasets/Accuracies/'\n",
        "        filename = filename + str(train_receptor) + '_newaccuracies' + str(runs) +  'ECFP4 10000.csv'\n",
        "        accuracy_df.to_csv(filename)\n",
        "\n",
        "        print(\"END\")\n",
        "\n",
        "# Combine all accuracies file for all runs\n",
        "root_dir = '/content/drive/My Drive/Applicability domains/Similarities/Similarity indicator/Machine learning/'\n",
        "root_dir = root_dir + '/- Scrubbing datasets/Accuracies/'\n",
        "\n",
        "for runs in range(1,26):\n",
        "    if runs == 1:\n",
        "        filename = root_dir + str(train_receptor) + '_newaccuracies' + str(runs) + 'ECFP4 10000.csv'\n",
        "        first_file = pd.read_csv(filename)\n",
        "        first_file = first_file.drop(['Unnamed: 0'],axis=1)\n",
        "        combined = first_file\n",
        "    if runs != 1:\n",
        "        filename = root_dir + str(train_receptor) + '_newaccuracies' + str(runs) +'ECFP4 10000.csv'\n",
        "        next_file = pd.read_csv(filename)\n",
        "        next_file = next_file.drop(['Unnamed: 0'],axis=1)\n",
        "        combined = pd.concat([combined,next_file], ignore_index=True)\n",
        "    \n",
        "    # Save combined file\n",
        "    print(combined)\n",
        "    filename = root_dir + str(train_receptor) + '_newaccuracies combined ECFP4 10000.csv'\n",
        "    combined.to_csv(filename)\n",
        "\n",
        "# approx 5-10 min per model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZ5TDawVJg22"
      },
      "source": [
        "## Count positives and negative labels in dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5A1vdFMiMHxs"
      },
      "source": [
        "\"\"\"\n",
        "Created Jun 2021\n",
        "\n",
        "Author: Marcus Wei How Wang\n",
        "Code available at: https://github.com/Goodman-lab/AD-transferability\n",
        "Please acknowledge the authors if using the code, whether partially or in full\n",
        "\"\"\"\n",
        "# Import the usual suspects\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "#====================================================================================#\n",
        "receptor = 'SERT'\n",
        "root_dir = '/content/drive/My Drive/Applicability domains/Similarities/Similarity indicator/Machine learning/'\n",
        "filename = root_dir + str(receptor) + '/'\n",
        "filename = filename + str(receptor) + '_train_fingerprints ECFP4 10000.csv'\n",
        "train_df = pd.read_csv(filename)\n",
        "\n",
        "# Count positive and negative labels in training set\n",
        "labels = train_df.iloc[:,-1]\n",
        "positive = 0\n",
        "negative = 0\n",
        "for x in range(0,len(labels)):\n",
        "    check = labels.iloc[x]\n",
        "    if check == 0:\n",
        "        negative += 1\n",
        "    if check == 1:\n",
        "        positive += 1\n",
        "\n",
        "print('\\nTraining dataset stats')\n",
        "print('Positives: {}'.format(positive))\n",
        "print('Negatives: {}'.format(negative))\n",
        "print('Total: {}'.format(len(labels)))\n",
        "print('Check Total: {}'.format(positive + negative))\n",
        "\n",
        "root_dir = '/content/drive/My Drive/Applicability domains/Similarities/Similarity indicator/Machine learning/'\n",
        "filename = root_dir + str(receptor) + '/'\n",
        "filename = filename + str(receptor) + '_test_fingerprints ECFP4 10000.csv'\n",
        "test_df = pd.read_csv(filename)\n",
        "\n",
        "# Count positive and negative labels in test set\n",
        "labels = test_df.iloc[:,-1]\n",
        "positive = 0\n",
        "negative = 0\n",
        "for x in range(0,len(labels)):\n",
        "    check = labels.iloc[x]\n",
        "    if check == 0:\n",
        "        negative += 1\n",
        "    if check == 1:\n",
        "        positive += 1\n",
        "\n",
        "print('\\nTest dataset stats')\n",
        "print('Positives: {}'.format(positive))\n",
        "print('Negatives: {}'.format(negative))\n",
        "print('Total: {}'.format(len(labels)))\n",
        "print('Check Total: {}'.format(positive + negative))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
